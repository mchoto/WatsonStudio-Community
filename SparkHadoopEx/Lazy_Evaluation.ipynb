{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://zerg1.fyre.ibm.com/FileShare/atlas_higgs.csv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Lazy Evaluation \n",
    "\n",
    "\n",
    "- spark.read : Will load the data schema/reference to Driver/Executors - No transofrm operations yet \n",
    "- df1.registerTempTable - Lazy Evaluation against original df. \n",
    "- df2 = spark.sql operations on df1 - Lazy Evaluation against df1\n",
    "- df2.show() / df2.count() / df2.collect() -> Execute the DAG/Slow process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.41 ms, sys: 7.42 ms, total: 15.8 ms\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = spark.read.csv('./atlas_higgs.csv', inferSchema = True, header = True)\n",
    "# df1 = spark.sql(\"SELECT * FROM csv.`./atlas_higgs.csv`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lazy Eval Operations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.13 ms, sys: 233 µs, total: 2.36 ms\n",
      "Wall time: 291 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1.registerTempTable(\"in_mem_tmp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.41 ms, sys: 1.96 ms, total: 5.37 ms\n",
      "Wall time: 156 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df2 = spark.sql(\"\"\"select EventId, DER_mass_MMC, DER_mass_vis \n",
    "                   from in_mem_tmp_table\n",
    "                   where EventId > 100000 and EventId < 120000 and DER_mass_MMC > 1\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.2 ms, sys: 728 µs, total: 4.92 ms\n",
      "Wall time: 7.07 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16972"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.71 ms, sys: 360 µs, total: 3.07 ms\n",
      "Wall time: 2.14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+\n",
      "|EventId|DER_mass_MMC|DER_mass_vis|\n",
      "+-------+------------+------------+\n",
      "| 100001|     160.937|     103.235|\n",
      "+-------+------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 2.32 ms, sys: 1.56 ms, total: 3.88 ms\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Note, spark.sql will treat the df= as a lazy evaluation step for non-file datasources (JDBC/Hive/etc):\n",
    "\n",
    "If the spark.read was spark.sql on an Existing Spark Datasource instead of csv, the \"load\" is optimized to Lazy Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 ms, sys: 1.13 ms, total: 2.14 ms\n",
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_connected_hive = spark.sql(\"select * from atlas_higgs_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.44 ms, sys: 3.34 ms, total: 9.78 ms\n",
      "Wall time: 1.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_connected_hive.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Python (YARN Cluster Mode)",
   "language": "python3",
   "name": "spark_python_yarn_cluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
