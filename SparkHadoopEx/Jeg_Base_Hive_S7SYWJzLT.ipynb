{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## JEG Base + Hive - Quick Dataframe exploration"}, {"metadata": {}, "cell_type": "code", "source": "spark", "execution_count": 1, "outputs": [{"output_type": "execute_result", "execution_count": 1, "data": {"text/plain": "<pyspark.sql.session.SparkSession at 0x7f17cc34a7b8>", "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cdhdemo4.fyre.ibm.com:39477\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.3.0.cloudera3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>58a33159-504e-4d44-b570-b758a341bc39</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"show tables\").show(10, truncate = False)", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "+--------+------------------------+-----------+\n|database|tableName               |isTemporary|\n+--------+------------------------+-----------+\n|default |atlas_higgs_100x        |false      |\n|default |atlas_higgs_100x_parquet|false      |\n|default |customer_history        |false      |\n|default |customers               |false      |\n|default |hive_test               |false      |\n|default |sample_07               |false      |\n|default |sample_071              |false      |\n|default |sample_0711             |false      |\n|default |sample_072              |false      |\n|default |sample_073              |false      |\n+--------+------------------------+-----------+\nonly showing top 10 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Native Hadoop Jupyter Shell Actions"}, {"metadata": {}, "cell_type": "code", "source": "# Cloudera Only \nimport os\nos.environ['HADOOP_CONF_DIR']=os.environ['HADOOP_CLIENT_CONF_DIR']", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!hostname -f", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "cdhdemo4.fyre.ibm.com\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "!hdfs dfs -ls /user/user1/atl*.csv", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "-rw-r--r--   3 user1 user1   55253673 2018-10-01 15:22 /user/user1/atlas_higgs.csv\r\n-rw-r--r--   3 user1 user1   55253673 2018-10-02 23:35 /user/user1/atlas_higgsx.csv\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<br>\n\n---\n\n### **SECURE.** hdfs shell access\n\n**Kerberos + Hadoop Access Controls enforced natively.** \n\nEx: `user1` should NOT be abllowed to read `user2`"}, {"metadata": {}, "cell_type": "code", "source": "!hdfs dfs -ls /user/user2", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "ls: Permission denied: user=user1, access=READ_EXECUTE, inode=\"/user/user2\":user2:user2:drwx------\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "---\n## Quickly and securly wrangle Data from Hdfs<->DF<->Hive"}, {"metadata": {}, "cell_type": "code", "source": "# Read raw data from hdfs\ndf1 = spark.read.csv(\"/user/user1/atlas_higgs.csv\",header=True)", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Tab Complete operations work via JEG**"}, {"metadata": {}, "cell_type": "code", "source": "df1.", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df1.createTempView(\"higgs_tmp\")", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# spark.sql(\"drop table atlas_higgs_demo\")", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Views can be persisted back to Hive as a new table\nspark.sql(\"create table atlas_higgs_demo as select * from higgs_tmp\")", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "DataFrame[]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df1.count()", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "250000"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Quickly Benchmark different Hive formats\n\nExtremely useful when working with Large Datasets. \n\n**Example:**\n\n25 Million Row Dataset, Using `%%time` to quickly benchmark spark reads"}, {"metadata": {}, "cell_type": "code", "source": "%%time\ndf_lg_parquet = spark.sql('select * from atlas_higgs_100x_parquet')\nprint(df_lg_parquet.count())", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "25000000\nCPU times: user 6.2 ms, sys: 1.94 ms, total: 8.15 ms\nWall time: 24.2 s\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "%%time\ndf_lg = spark.sql('select * from atlas_higgs_100x')\nprint(df_lg.count())", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "25000000\nCPU times: user 6.09 ms, sys: 15.3 ms, total: 21.4 ms\nWall time: 22.2 s\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n## Summary\n\n- Quick Data Wrangling Experience\n- Native Jupyter Magics/Actions empower HDFS/Kerberos experience\n\n\n---\n\n#### Additional Resources/DataGen Notes \n\n- Creating 100x table with 25M rows"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"create table atlas_higgs_100x as (select * from higgs_tmp)\")\nfor x in range(1,99):\n    spark.sql(\"insert into atlas_higgs_100x ( select * from atlas_higgs)\")", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%time\ndf_sm = spark.sql('select * from atlas_higgs')\nprint(df_sm.count())", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "250000\nCPU times: user 1.31 ms, sys: 937 \u00b5s, total: 2.25 ms\nWall time: 480 ms\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "%%time\ndf_lg = spark.sql('select * from atlas_higgs_100x')\nprint(df_lg.count())", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "24750000\nCPU times: user 3.5 ms, sys: 1.78 ms, total: 5.27 ms\nWall time: 23.2 s\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Create same table, stored as parquet"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"create table atlas_higgs_100x_parquet stored as parquet as select * from atlas_higgs_100x\")", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "DataFrame[]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"drop table atlas_higgs_demo\")", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "spark_python_yarn_cluster", "display_name": "Spark - Python (YARN Cluster Mode)", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.5.6", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat": 4, "nbformat_minor": 1}