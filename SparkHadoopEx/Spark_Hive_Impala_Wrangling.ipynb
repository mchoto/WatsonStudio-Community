{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Hive Examples - Hadoop Execution Engine\n",
    "\n",
    "- JEG Notebook Kernels will automatically talk to Hive Metastore\n",
    "- In CDH, Impala Metastore access \n",
    "  - Impala Can be used to create tables on different data sources! E.g. Kudu, HDFS\n",
    "- Spark \"Temporary\" tables vs Hive tables for spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.51 ms, sys: 5.19 ms, total: 13.7 ms\n",
      "Wall time: 6.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[database: string, tableName: string, isTemporary: boolean]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Note - spark.sql is a lazy operation! Will not display the actual query result\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |atlas_higgs_100x        |false      |\n",
      "|default |atlas_higgs_100x_parquet|false      |\n",
      "|default |atlas_higgs_demo        |false      |\n",
      "|default |customer_history        |false      |\n",
      "|default |customers               |false      |\n",
      "|default |hive_test               |false      |\n",
      "+--------+------------------------+-----------+\n",
      "only showing top 6 rows\n",
      "\n",
      "CPU times: user 4.87 ms, sys: 517 µs, total: 5.39 ms\n",
      "Wall time: 926 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"show tables\").show(6, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark different Hive formats\n",
    "\n",
    "\n",
    "**Example:** Query 4 columns from a 25 Million Row Dataset.\n",
    "\n",
    "**- Table1: atlas_higgs_100x default hive storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 ms, sys: 1.26 ms, total: 2.79 ms\n",
      "Wall time: 335 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_table1 = spark.sql(\"\"\"select EventId, DER_mass_MMC, DER_mass_vis, DER_sum_pt \n",
    "                    from atlas_higgs_100x\n",
    "                    where EventId > 100000 and EventId < 120000 and DER_mass_MMC > 1\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 2.6 ms, total: 12.9 ms\n",
      "Wall time: 51.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1697200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_table1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Table2: atlas_higgs_100x_parquet hive table stored as parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 ms, sys: 2.51 ms, total: 4.17 ms\n",
      "Wall time: 733 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_table2 = spark.sql(\"\"\"select EventId, DER_mass_MMC, DER_mass_vis \n",
    "                    from atlas_higgs_100x_parquet\n",
    "                    where EventId > 100000 and EventId < 120000 and DER_mass_MMC > 1\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.18 ms, sys: 1.67 ms, total: 3.85 ms\n",
      "Wall time: 6.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1697200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_table2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "Hive: 51seconds vs Hive+Parquet: 6.7seconds !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 - \"Registering temp tables\" and spark.sql scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 ms, sys: 246 µs, total: 2.64 ms\n",
      "Wall time: 67.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_table2.registerTempTable(\"tmp_table_dftable2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, truncate=False to display wide tables / not truncate output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |atlas_higgs_100x        |false      |\n",
      "|default |atlas_higgs_100x_parquet|false      |\n",
      "|default |atlas_higgs_demo        |false      |\n",
      "|default |customer_history        |false      |\n",
      "|default |customers               |false      |\n",
      "|default |hive_test               |false      |\n",
      "|default |kudu_external_t1        |false      |\n",
      "|default |sample_07               |false      |\n",
      "|default |sample_071              |false      |\n",
      "|default |sample_0711             |false      |\n",
      "|default |sample_072              |false      |\n",
      "|default |sample_073              |false      |\n",
      "|default |sample_08               |false      |\n",
      "|default |sample_082              |false      |\n",
      "|default |t1                      |false      |\n",
      "|default |t2                      |false      |\n",
      "|default |test001                 |false      |\n",
      "|default |web_logs                |false      |\n",
      "|        |tmp_table_dftable2      |true       |\n",
      "+--------+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, spark.sql can at this point query Hive/Impala/Kudu tables, as well as the isTemporary table we registered to memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Python (YARN Cluster Mode)",
   "language": "python3",
   "name": "spark_python_yarn_cluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
