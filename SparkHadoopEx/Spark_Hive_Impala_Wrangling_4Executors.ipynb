{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Hive Examples - Hadoop Execution Engine\n",
    "\n",
    "- JEG Notebook Kernels will automatically talk to Hive Metastore\n",
    "- In CDH, Impala Metastore access \n",
    "  - Impala Can be used to create tables on different data sources! E.g. Kudu, HDFS\n",
    "- Spark \"Temporary\" tables vs Hive tables for spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ms, sys: 2.54 ms, total: 13.6 ms\n",
      "Wall time: 7.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[database: string, tableName: string, isTemporary: boolean]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Note - spark.sql is a lazy operation! Will not display the actual query result\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |atlas_higgs_100x        |false      |\n",
      "|default |atlas_higgs_100x_parquet|false      |\n",
      "|default |atlas_higgs_demo        |false      |\n",
      "|default |customer_history        |false      |\n",
      "|default |customers               |false      |\n",
      "|default |hive_test               |false      |\n",
      "+--------+------------------------+-----------+\n",
      "only showing top 6 rows\n",
      "\n",
      "CPU times: user 3.38 ms, sys: 637 µs, total: 4.01 ms\n",
      "Wall time: 737 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"show tables\").show(6, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark different Hive formats\n",
    "\n",
    "\n",
    "**Example:** Query 4 columns from a 25 Million Row Dataset.\n",
    "\n",
    "**- Table1: atlas_higgs_100x default hive storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 ms, sys: 232 µs, total: 2.59 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_table1 = spark.sql(\"\"\"select EventId, DER_mass_MMC, DER_mass_vis, DER_sum_pt \n",
    "                    from atlas_higgs_100x\n",
    "                    where EventId > 100000 and EventId < 120000 and DER_mass_MMC > 1\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.85 ms, sys: 3.49 ms, total: 12.3 ms\n",
      "Wall time: 46.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1697200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_table1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Table2: atlas_higgs_100x_parquet hive table stored as parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 ms, sys: 1.58 ms, total: 3.84 ms\n",
      "Wall time: 755 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_table2 = spark.sql(\"\"\"select EventId, DER_mass_MMC, DER_mass_vis \n",
    "                    from atlas_higgs_100x_parquet\n",
    "                    where EventId > 100000 and EventId < 120000 and DER_mass_MMC > 1\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.57 ms, sys: 1.53 ms, total: 11.1 ms\n",
      "Wall time: 5.97 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1697200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_table2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "1 Executors: Hive: 51seconds vs Hive+Parquet: 6.7seconds \n",
    "\n",
    "4 Executors: Hive: 46seconds vs Hive+Parquet: 5.9seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 - \"Registering temp tables\" and spark.sql scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 ms, sys: 246 µs, total: 2.64 ms\n",
      "Wall time: 67.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_table2.registerTempTable(\"tmp_table_dftable2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, truncate=False to display wide tables / not truncate output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |atlas_higgs_100x        |false      |\n",
      "|default |atlas_higgs_100x_parquet|false      |\n",
      "|default |atlas_higgs_demo        |false      |\n",
      "|default |customer_history        |false      |\n",
      "|default |customers               |false      |\n",
      "|default |hive_test               |false      |\n",
      "|default |kudu_external_t1        |false      |\n",
      "|default |sample_07               |false      |\n",
      "|default |sample_071              |false      |\n",
      "|default |sample_0711             |false      |\n",
      "|default |sample_072              |false      |\n",
      "|default |sample_073              |false      |\n",
      "|default |sample_08               |false      |\n",
      "|default |sample_082              |false      |\n",
      "|default |t1                      |false      |\n",
      "|default |t2                      |false      |\n",
      "|default |test001                 |false      |\n",
      "|default |web_logs                |false      |\n",
      "|        |tmp_table_dftable2      |true       |\n",
      "+--------+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, spark.sql can at this point query Hive/Impala/Kudu tables, as well as the isTemporary table we registered to memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Python (YARN Cluster Mode)",
   "language": "python3",
   "name": "spark_python_yarn_cluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
