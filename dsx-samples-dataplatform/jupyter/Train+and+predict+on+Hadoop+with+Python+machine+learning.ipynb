{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Using Watson Studio Machine Learning Service for Model Training and Making Predictions on Hadoop Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook shows you how to use python machine learning libraries and services from Watson Studio to train, evaluate, and save a model on a remote Hadoop cluster.\n",
    "\n",
    "Our input data will reside in HDFS for a registered Hadoop Integration system. To avoid having to copy the data from Hadoop into Watson Studio, we will use a remote Livy session to build the model _within Hadoop itself_. Then we will \"pull\" the model into Watson Studio and save it to your Watson Studio filesystem, making it available for use with other Watson Studio model management features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">Note: In this exercise we will be using Spark from a <i>remote</i> Hadoop session to build the model, and then we will use Spark in the <i>local</i> notebook to load the model.  This means that your remote Hadoop cluster **must** be running with a version of Spark that is compatible with the version of Spark that you're using for this notebook. So if your remote Spark version is 2.0, you will need to run this notebook with Python 2.7; if your remote Spark version is 2.1 or 2.2, you'll need to run this notebook with Python 3.5.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Table of contents\n",
    "- [Prerequisites (Admin)](#prerequisites)\n",
    "- [Create a Remote Livy Session](#create_livy_session)\n",
    "- [Load Data](#load_data)\n",
    "- [Access and Manipulate Data](#access_manipulate_data)\n",
    "- [Evaluate the Model](#evaluate_model)\n",
    "- [Copy the Model to Watson Studio Local](#model_copy_to_wsl)\n",
    "- [Save the Model](#save_model)\n",
    "- [Cleanup the Remote Livy Session](#cleanup_livy_session)\n",
    "- [Summary](#summary)\n",
    "\n",
    "<a id='prerequisites'></a>\n",
    "## Prerequisites (Admin)\n",
    "\n",
    "In order to run Livy sessions on a remote Hadoop cluster, your Watson Studio admin must first register a Hadoop Integration system with Watson Studio.\n",
    "\n",
    "Ask your Watson Studio admin to use the **Admin Console => Hadoop Integration** option to register a Hadoop Integration system. ** NOTE: Installation and configuration of IBM's Hadoop Integration (`HI`) service on a Hadoop cluster must be done by a Hadoop admin _before_ that system can be registered with your Watson Studio account. **\n",
    "\n",
    "When your admin indicates that a Hadoop Integration system has been registered, you can proceed with this sample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports needed for the cells which run locally on Watson Studio.\n",
    "import dsx_core_utils\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='create_livy_session'></a>\n",
    "## Create a Remote Livy Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, let's get a list of registered Hadoop Integration systems. For this example, we're running in the remote Spark and we do not require any special python libraries, so we do **not** need to look for any particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DSXHI_SYSTEMS = dsx_core_utils.get_dsxhi_info(showSummary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Configure the Spark session that we will run on the selected registered HI system. In this case we want the session to start with 1G memory and two Spark executors. **NOTE**: `myConfig` here is optional; if you prefer to use default configs you can omit this cell and remove the `addlConfig` argument in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"1G\",\n",
    " \"numExecutors\": 2\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://pbody-edge-1.fyre.ibm.com:8443/gateway/dsx-loc-chell-375-master-1/livy2/v1 \n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "# Set up sparkmagic to connect to the selected registered HI\n",
    "# system with the specified configs. **NOTE** This notebook\n",
    "# requires Spark 2, so you should set 'livy' to 'livyspark2'.\n",
    "HI_CONFIG = dsx_core_utils.setup_livy_sparkmagic(\n",
    "  system=\"P-Body\", \n",
    "  livy=\"livyspark2\",\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load sparkmagic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's capture some state about the configured Hadoop Integraton system, to be used later in this notebook. Then start up a new, remote Livy session to connect to that HI system. **NOTE**: Depending on a) the resources available in the remote Hadoop system and b) the speed of your cluster, attempts to start the session might report errors due to timeout or due to a session coming up `dead`.  In such cases you should run **`%spark cleanup`** as a separate cell, then re-run this cell again.  If session creation continues to fail, contact the Hadop admin of the target Hadoop cluster to see if everything is configured as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>271</td><td>application_1536856451431_0224</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hdp-264-pbody-2.fyre.ibm.com:8088/proxy/application_1536856451431_0224/\">Link</a></td><td><a target=\"_blank\" href=\"http://hdp-264-pbody-1.fyre.ibm.com:8042/node/containerlogs/container_e14_1536856451431_0224_01_000001/user1\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'mlsess1'\n",
    "livy_endpoint = HI_CONFIG['LIVY']\n",
    "webhdfs_endpoint = HI_CONFIG['WEBHDFS']\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For reference / debugging: Print out the name of the Hadoop node to which the remote session has been assigned. When \"local\" files are created within the remote session, they will be written to this node. All of the Yarn container artifacts (workspace and temp files) will exist on this node, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote livy session driver: hdp-264-pbody-1.fyre.ibm.com"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "import socket\n",
    "print(\"Remote livy session driver: {}\".format(socket.gethostname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following cell, and all subsequent cells which have **`%%spark`** as their first line, will run *remotely*, i.e. within a Yarn container that exists on the registered Hadoop Integration system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "# Declare imports needed for all of the cells that will run remotely.\n",
    "import getpass, time, os, shutil\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load IBM Hadoop Integration utilities to facilitate remote functionality.\n",
    "# This line assumes that HI version >= X.Y has been installed on the registered\n",
    "# Hadoop Integration system.\n",
    "hi_utils_lib = os.getenv(\"HI_UTILS_PATH\", \"\")\n",
    "sc.addPyFile(\"hdfs://{}\".format(hi_utils_lib))\n",
    "import hi_core_utils\n",
    "\n",
    "# Declare a target HDFS directory path that will be used for our data.\n",
    "hdfs_dataset_dir = \"/user/{}/datasets\".format(getpass.getuser())\n",
    "input_ds = \"{}/{}\".format(hdfs_dataset_dir, \"cars.csv\")\n",
    "\n",
    "# Create target hdfs directory, if it does not already exist.\n",
    "hi_core_utils.run_command(\"hdfs dfs -mkdir -p {}\".format(hdfs_dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"load_data\"></a>\n",
    "## Load Data \n",
    "The 1983 Data Exposition dataset was collected by Ernesto Ramos and David Donoho and dealt with automobiles. Data on mpg, cylinders, displacement, was provided for 406 different cars, each identified by name. The dataset is freely available on the Watson Studio home page.\n",
    "\n",
    "Perform the following steps to upload this dataset:\n",
    "1. Go to the <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/c81e9be8daf6941023b9dc86f303053b\" target=\"_blank\">Car performance data</a> card on the Watson Studio home page.\n",
    "1. Click the download button.\n",
    "1. Click the **Create new** icon on the notebook action bar, and use the **Add data set** button to add the downloaded cars.csv file as a `Local File`. \n",
    "\n",
    "The data file is listed on the **Local Data** pane in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's load our test data into HDFS. For the purposes of this sample, our data is small and comes from the local `cars.csv` file created above. We do not _need_ to put it into HDFS for this example--but we choose to do so for demonstration purposes. In a real scenario the desired data should already be loaded into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload success\n"
     ]
    }
   ],
   "source": [
    "# Redeclare the dataset dir locally--the earlier declaration was in the _remote_\n",
    "# session so it is not available here.\n",
    "\n",
    "# ** NOTE ** Replace {your-username-here} with your actual user name.\n",
    "hdfs_dataset_dir = \"/user/{your-username-here}/datasets\"\n",
    "\n",
    "# Upload the saved csv file from Local to the remote HDFS.\n",
    "input_csv = os.environ[\"DSX_PROJECT_DIR\"] + \"/datasets/cars.csv\"\n",
    "dsx_core_utils.upload_hdfs_file(webhdfs_endpoint, input_csv, \"{}/cars.csv\".format(hdfs_dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"access_manipulate_data\"></a>\n",
    "## Access and Manipulate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now use Spark to read the data, as a **Spark dataframe**, from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+----------+------+------------+----+--------+--------------------+\n",
      "|mpg|cylinders|engine|horsepower|weight|acceleration|year|  origin|                name|\n",
      "+---+---------+------+----------+------+------------+----+--------+--------------------+\n",
      "| 18|        8| 307.0|       130|  3504|        12.0|  70|American|chevrolet chevell...|\n",
      "| 15|        8| 350.0|       165|  3693|        11.5|  70|American|   buick skylark 320|\n",
      "| 18|        8| 318.0|       150|  3436|        11.0|  70|American|  plymouth satellite|\n",
      "| 16|        8| 304.0|       150|  3433|        12.0|  70|American|       amc rebel sst|\n",
      "| 17|        8| 302.0|       140|  3449|        10.5|  70|American|         ford torino|\n",
      "+---+---------+------+----------+------+------------+----+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "df_data_0 = spark.read.format(\n",
    "    \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\n",
    "    \"header\", \"true\").option(\"inferSchema\", \"true\").load(input_ds)\n",
    "\n",
    "df_data_0.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Due to missing data in the `mpg` and `horsepower` columns, they will be excluded from the dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------------+----+--------+--------------------+\n",
      "|cylinders|engine|weight|acceleration|year|  origin|                name|\n",
      "+---------+------+------+------------+----+--------+--------------------+\n",
      "|        8| 307.0|  3504|        12.0|  70|American|chevrolet chevell...|\n",
      "|        8| 350.0|  3693|        11.5|  70|American|   buick skylark 320|\n",
      "|        8| 318.0|  3436|        11.0|  70|American|  plymouth satellite|\n",
      "|        8| 304.0|  3433|        12.0|  70|American|       amc rebel sst|\n",
      "|        8| 302.0|  3449|        10.5|  70|American|         ford torino|\n",
      "+---------+------+------+------------+----+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "carsDataRaw = df_data_0\n",
    "carsModData = carsDataRaw.drop(\"mpg\").drop(\"horsepower\")\n",
    "carsModData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the model training process, the original dataset will be split into a training dataset and a testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training dataset: 348\n",
      "Number of testing dataset: 58"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "splitted_data = carsModData.randomSplit([0.85, 0.15], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training dataset: {}\".format(train_data.count()))\n",
    "print(\"Number of testing dataset: {}\".format(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now set the input columns for model training, and use the corresponding algorithms to train the model. In this example, the Linear Regression method is used to evaluate `weight` in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "originIndexer = StringIndexer().setInputCol(\"origin\").setOutputCol(\"origin_code\")\n",
    "vectorAssembler_features = VectorAssembler().setInputCols(\n",
    "    [\"cylinders\", \"engine\", \"acceleration\", \"year\", \"origin_code\"]).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "rf = LinearRegression().setLabelCol(\"weight\").setFeaturesCol(\"features\")\n",
    "pipeline = Pipeline().setStages([originIndexer,vectorAssembler_features,rf])\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"evaluate_model\"></a>\n",
    "## Evaluate the Model\n",
    "The model performance can be evaluated using the R Square for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Square of Test Data: 0.863976844308"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "testData = model.transform(test_data).drop(\"prediction\")\n",
    "metric = model.stages[2].evaluate(testData)\n",
    "print(\"R Square of Test Data: {}\".format(metric.r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='model_copy_to_wsl'></a>\n",
    "## Copy the Model to Watson Studio Local\n",
    "\n",
    "The model now exists within the memory of the remote livy session. In order to use it in Watson Studio model management, we need to copy it to the local Watson Studio environment.  This is done in two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Write the model to HDFS\n",
    "First, in the _remote_ session, we use a Hadoop Integration utility method to write the model to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'hdfs:///user/user1/.dsxhi/models/ml_cars_model/3/model', 'version': 3, 'name': 'ml_cars_model', 'latest_version': 3}"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "hi_core_utils.write_model_to_hdfs(model=model, model_name=\"ml_cars_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the model from HDFS into Watson Studio\n",
    "Then, on the Watson Studio _local_ side, use a Watson Studio utility method to load the model from HDFS into memory. Note that the model name we use here should match the one we used in the previous cell, when we wrote the model to HDFS.\n",
    "\n",
    "Note also that this cell **does not** begin with the **`%%spark`** line, which means it is running locally in your Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from hdfs:///user/user1/.dsxhi/models/ml_cars_model/3/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "ml_cars = dsx_core_utils.load_model_from_hdfs(webhdfs_endpoint, model_name=\"ml_cars_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='save_model'></a>\n",
    "## Save the Model\n",
    "We can now save the Spark model to the Watson Studio filesystem for publishing, scoring, deployment, and evaluations.\n",
    "\n",
    "When invoking the `save` function we want to pass a pandas dataframe for **`test_data`** as an argument. By doing so we allow the `save` function to a) determine the schema of the test data automatically, and b) find an example row that can be used elsewhere in the WSL model management UI (ex. for real-time scoring).\n",
    "\n",
    "At this point the desired dataframe exists within the _remote_ Livy session, which means it is not directly accessible from the local notebook session. However, we can use `sparkmagic` to pull a **single** row (\"`-n 1`\") from the remote dataframe.  This allows us to get the minimum necessary information we need from the test data **without** having to read the full datasets from HDFS.\n",
    "\n",
    "Here we load one row of data from the remote dataframe into a local dataframe named **`cars_test_data`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name -n 1 -o cars_test_data\n",
    "cars_test_data = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above cell will load the data as a pandas dataframe, **`cars_test_data`**, but the `save` call below needs it to be a Spark dataframe since we're dealing with a Spark model. So we have to convert it into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "test_data = SQLContext(sc).createDataFrame(cars_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have our **`test_data`** dataframe, let's import the `save` function from the `dsx_ml.ml` library and save the model.\n",
    "\n",
    "**NOTE**: Since we're using a dataframe with a **single** row, i.e. partial data, we choose to skip calculation of performance metrics for the saved model (\"`skip_metrics = True`\") since metrics based on a single row are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'path': '/user-home/1001/DSX_Projects/Models on Hadoop/models/Cars ML via Hadoop/22',\n",
       " 'scoring_endpoint': 'https://dsxl-api/v3/project/score/Python35/spark-2.2/Models%20on%20Hadoop/Cars%20ML%20via%20Hadoop/22'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dsx_ml.ml import save\n",
    "save(name='Cars ML via Hadoop', model=ml_cars, test_data=test_data, algorithm_type='Regression', skip_metrics = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='cleanup_livy_session'></a>\n",
    "## Cleanup the Remote Livy Session\n",
    "We're done with our models and we have successfully saved them to Watson Studio. Let's clean up our remote Livy session. \n",
    "This will terminate the session and release resources back to the remote Hadoop Integration system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='summary'></a>\n",
    "## Summary\n",
    "In this notebook you learned how to create a Spark model using machine learning libraries _on a registered Hadoop Integration system_, allowing you to create the model where the data resides, instead of having to copy your data into the Watson Studio environment.  Once the model was created you were able to save it in the Watson Studio environment, where it can now be used as input for other Watson Studio model management features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">Note: To save resources and get the best performance please use the code below to stop the kernel before exiting your notebook.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2018. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
