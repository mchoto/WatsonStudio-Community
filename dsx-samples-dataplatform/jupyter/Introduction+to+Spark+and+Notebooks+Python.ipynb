{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction to Spark and Notebooks - Python\n",
    "This notebook is a live tutorial that introduces notebooks and some basic Spark operations.\n",
    "\n",
    "The goal of this tutorial is to get productive with notebooks as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is a Notebook?\n",
    "A **notebook** is a web application that enables the creation of documents that include executable code.<br/>\n",
    "This is why a notebook includes a kernel that defines the programming language and Spark engine used. \n",
    "\n",
    "It starts with the concept of a **cell**: A Cell can either be formatted text (Markdown), executable code (Code), or Raw which can contain anything.<br/>\n",
    "`Markdown` and `Code` are the basic cell element we will be working with throughout this lab.\n",
    "\n",
    "You can see menus and buttons at the top of the screen. The main menus are:<br/>\n",
    "`File`, `Edit`, `View`, `Insert`, `Cell`, `Kernel`, and `Help`\n",
    "\n",
    "Under the menus, we see a list of buttons. You can hover the mouse on top of them to get an idea of what they are used for.\n",
    "\n",
    "You should take some time to look at the menu options. We'll mention some of them as we go through this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Executing code\n",
    "It's time to execute our first cell. The executable cell below is preceded by __`In [ ]:`__\n",
    "\n",
    "* Click on the cell below to make sure it is selected\n",
    "* Find the __`run cell`__ button in the buttons above\n",
    "* Click the run cell button\n",
    "\n",
    "You should see the __`In [ ]`__ become __`In [*]`__ and finally __`In [1]`__.\n",
    "\n",
    "When you see __`In [*]`__, it means that the code is executing.<br/>\n",
    "Once it becomes a number, the execution is completed. The number is the sequence in multiple executions.<br/>\n",
    "In our case, it is the first cell we executed so it is __`[1]`__\n",
    "\n",
    "Some cells may not return a result. The execution sequence will indicate that it is done executing.\n",
    "\n",
    "The cell below gets a Spark session and displays the kernel version.<br/>\n",
    "A Spark session is the basic interface with the Spark kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Reading a file\n",
    "One fundamental operation is to access existing data. Spark has the ability to read from many sources including files and databases.<br/>\n",
    "When reading from files, it can accommodate files that are compressed or not, in csv, json, and parquet format, and more.\n",
    "\n",
    "Since Spark uses lazy evaluation. This means that if a result does not need to be computed, Spark simply notes what needs to be odne and waits to execute it until it is really needed. In thss case, the result is a dataframe that is a reference to what needs to be done to access the data.\n",
    "\n",
    "In the following example we read a compressed file that is in json format. The `count` method forces the evaluation and displays the record count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load('../datasets/world_bank.json.gz')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RDDs or DataFrames?\n",
    "These two elements are representations of data. You may have heard of RDDs: Resilient Distributed Dataset. RDDs are the foundation to data maipulation in Spark. Over time, DataFrames were added to provide more functionality and optimization opportunities.\n",
    "\n",
    "In general, you'll want to stay with the manipulation of DataFrames. DataFrames provide access to the underlying RDDs, so if necessary you can still go to that level of manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DataFrame schema\n",
    "We can look at the data definition to get a better idea of what we want to do with it.\n",
    "\n",
    "Execute the following cell to see the dataframe schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Code completion\n",
    "It is impossible to remember all the methods available for a specific class.<br/>\n",
    "Let's take the DataFrame we created earlier. How can we know there is a `printSchema` method?\n",
    "\n",
    "We can keep a browser window open on the documentation and seach for what we want or we can use the `tab` key.\n",
    "\n",
    "Try it in the cell below with the following. Type __`df.p`__ and then __press the tab key__.<br/>\n",
    "You should see a selection drop down menu appear where you can select the one you want.\n",
    "\n",
    "How can you find out the names of the attributes that are part of the DataFrame? Hint, it starts with \"c\".<br/>\n",
    "Try it and find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df.p\n",
    "# tab completion should list df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Manipulating DataFrames\n",
    "We already saw the use of the __`count()`__ method on DataFrames. There are many methods available. In this section we explore some of these methods. For further information, you need to refer to the <a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\">Spark documentation</a>.\n",
    "\n",
    "The following methods are used to extract rows (records) form a Dataframe:\n",
    "* __collect()__ - return the entire content of a DataFrame\n",
    "* __first()__   - Return the first row of a DataFrame\n",
    "* __head(n)__   - Return the first n rows of a DataFrame\n",
    "* __take(n)__   - Return the first n rows of a DataFrame\n",
    "\n",
    "The __`collect()`__ method can be dangerous since it could return the entire content of a DataFrame which could represent many millions of records. In can be used in conjunction with the __`limit()`__ method that would reduce the number of records returned, turning the __`limit(n).collect()`__ into a __`head(n)`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Limit the dataframe to two rows then collect them\n",
    "df.limit(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# return the first two rows\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data exploration\n",
    "An important aspect of data science is the exploration of the data to achieve a better understanding. Several of these methods can be used together to refine the exploration:\n",
    "\n",
    "* __`describe(cols)`__ - provide basic statistics on the provided columns\n",
    "* __`filter(conditions)`__ - select specific rows based on column values (also called select)\n",
    "\n",
    "The next few cells show some examples of what could be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Distribution of values for grantamt\n",
    "df.describe(['grantamt']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Distribution of values for grantamt in Africa\n",
    "df.filter(\"countryname = 'Africa'\").describe(['grantamt']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# How many grantamt are zero\n",
    "df.filter(\"grantamt = 0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Distribution of grant amounts that are not zero\n",
    "df.filter(\"grantamt != 0\").describe(['grantamt']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Splitting data and sampling\n",
    "One important operation in data science is to work with samples to speed up our testing and to split a dataset into training, testing and validation sets. These operations are directly available using:\n",
    "\n",
    "* __`randomSplit()`__\n",
    "* __`sample`__\n",
    "* __`sampleBy()`__\n",
    "\n",
    "Check out the following examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the three DataFrames in one operation with split at 70%, 15%, 15% approximately\n",
    "(training_df, testing_df, valid_df) = df.randomSplit([70.0, 15.0, 15.0], 41)\n",
    "print (\"Training set count:   \" + str(training_df.count()) )\n",
    "print (\"Testing set count:    \" + str(testing_df.count()) )\n",
    "print (\"Validation set count: \" + str(valid_df.count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Take a random sample (without replacement) of around 20% of the data\n",
    "random_df = df.sample(False, 0.2)\n",
    "random_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Writing data out\n",
    "For completeness, we need to quickly cover writing data out.\n",
    "\n",
    "A `DataFrame` is a distributed data set. It is expected to live on multiple nodes in the cluster at the same time.<br/>\n",
    "For this reason, the default write method is expected to save it in a distributed matter if we are to save it as a file.<br/>\n",
    "This means the write method returns a `DataFrameWriter` that would then expect a distributed file system where a file is saved as a directory that contains multiple parts of the file. Of course, if a DataFrame consist of only one partition, the directory wil contain only one part.\n",
    "\n",
    "It is also possible to save the data into a database using a `jdbc` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Executing SQL queries\n",
    "Dataframes provide an SQL-like interface to the data. It is also possible to define a reference to a view so we can execute SQL statements directly on the data. What is returned is a DataFrame.\n",
    "\n",
    "In this example, we simply display the result to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"world_bank\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT id, borrower\n",
    "FROM world_bank\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Displaying results using Pandas\n",
    "The Python Pandas library is included in the environment. It can be used to provide a nicer format to the output.<br/>\n",
    "To enable this functionality, we simply import the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT  regionname, count(*) as project_count\n",
    "FROM     world_bank\n",
    "GROUP BY regionname \n",
    "ORDER BY count(*) DESC\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More on Pandas\n",
    "Pandas is a Python data analysis toolkit.\n",
    "\n",
    "When we convert a Spark DataFrame to a Pandas DataFrame, we basically read the DataFrame into our local memory. For this reason, a Spark DataFrame should be converted to a Pandas DataFrame only when it is small, like in the case of an aggregate operation like the one we just saw.\n",
    "\n",
    "The Pandas toolkit includes a lot of functionality and is beyond the scope of this introduction but ine part of it should be known right away: the ability to plot the content of a DataFrame with the help of matplotlib.<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pddf = spark.sql(\"\"\"\n",
    "SELECT  regionname, count(*) as project_count\n",
    "FROM     world_bank\n",
    "GROUP BY regionname \n",
    "ORDER BY count(*) DESC\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the previous result as a pie chard showing percentages of total.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(16,8))\n",
    "ax1 = plt.subplot(121, aspect='equal')\n",
    "\n",
    "pddf.plot(kind='pie', y='project_count', ax = ax1, autopct='%1.1f%%', \n",
    " startangle=90, shadow=False, labels=pddf['regionname'], legend = False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Saving a notebook\n",
    "Watson Studio will automatically save your notebook from time to time.<br/>\n",
    "If  you have a lot of changes, you may want to explicitly save your notebook. For this, you can either use the save icon (looking like a floppy disk) under the __`File`__ menu at the top of the screen or you can click on __`File`__ and select __\"Save and Checkpoint\"__.\n",
    "\n",
    "You can also download your saved notebook to your local machine using the __\"Download as\"__ option in the __`File`__ menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Congratulations!\n",
    "You've completed the introductory tutorial.<br/>\n",
    "Explore the other tutorials for more in-depth knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
