{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dist Keras running on Remote YARN environment, leveraging Watson Studio python libraries \n",
    "\n",
    "---\n",
    "\n",
    "Watson Studio Hadoop Integration allows users to securely connect to a remote HDP/CDH environment. One of the features Watson Studio Hadoop Integration provides, is for the Watson Studio admin to \"push\" a Runtime Environment to a HDFS user-readable location. \n",
    "\n",
    "When users start their remote livy sessions, they have the ability to select one of pushed image runtimes, as well as define custom spark configurations for a session.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 0. Prepare Livy/Spark Session Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Hadoop systems: \n",
      "\n",
      "   systemName  LIVYSPARK  LIVYSPARK2                  imageId\n",
      "0  cdh513mjou  livyspark                                     \n",
      "1       bendy  livyspark  livyspark2                         \n",
      "2    asgardia             livyspark2  dsx-scripted-ml-python2\n",
      "3      becks1             livyspark2                         \n",
      "4     yingcdh  livyspark  livyspark2                         \n",
      "5       zinc1  livyspark  livyspark2  dsx-scripted-ml-python2\n",
      "6       kabob             livyspark2                         \n",
      "7       matzo             livyspark2                         \n"
     ]
    }
   ],
   "source": [
    "# Show registered Hadoop Systems, and associated images \n",
    "import dsx_core_utils\n",
    "DSXHI_SYSTEMS = dsx_core_utils.get_dsxhi_info(showSummary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://asgardian-edge.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/livy2/v1 with image Jupyter with Python 2.7, Scala 2.11, R 3.4.3\n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "# Additional spark configs can be provided, as per https://github.com/apache/incubator-livy/blob/master/docs/rest-api.md\n",
    "myConfig={\"queue\": \"default\",\n",
    "         \"driverMemory\":\"3G\",\n",
    "         \"numExecutors\":3\n",
    "         }\n",
    "\n",
    "# Setup necessary session properties\n",
    "dsx_core_utils.setup_livy_sparkmagic(\n",
    "    system=\"asgardia\", \n",
    "    livy=\"livyspark2\", \n",
    "    imageId=\"dsx-scripted-ml-python2\",\n",
    "    addlConfig=myConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>98</td><td>application_1533834262477_0022</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://shad2.fyre.ibm.com:8088/proxy/application_1533834262477_0022/\">Link</a></td><td><a target=\"_blank\" href=\"http://shad4.fyre.ibm.com:8042/node/containerlogs/container_e11_1533834262477_0022_01_000001/user1\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Start a remote livy session, livy endpoint is displayed in the last cell\n",
    "session_name = 'DistKeras_Sample'\n",
    "livy_endpoint = \"https://asgardian-edge.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/livy2/v1\"\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running Spark:\n",
      "    Sessions:\n",
      "        Name: DistKeras_Sample\tSession id: 98\tYARN id: application_1533834262477_0022\tKind: pyspark\tState: idle\n",
      "\tSpark UI: http://shad2.fyre.ibm.com:8088/proxy/application_1533834262477_0022/\n",
      "\tDriver Log: http://shad4.fyre.ibm.com:8042/node/containerlogs/container_e11_1533834262477_0022_01_000001/user1\n",
      "    Session configs:\n",
      "        {'queue': 'default', 'numExecutors': 3, 'archives': ['/user/dsxhi/environments/7d47bdd5b4037a18ccfef3afd7e7399ed1859fae8a3c92588783aea56c341095/dsx-scripted-ml-python2.tar.gz'], 'conf': {'spark.yarn.appMasterEnv.PYSPARK_PYTHON': 'dsx-scripted-ml-python2.tar.gz/anaconda2/bin/python2.7', 'spark.yarn.appMasterEnv.PYTHONPATH': 'dsx-scripted-ml-python2.tar.gz/usr/local/spark-2.0.2-bin-hadoop2.7/python:dsx-scripted-ml-python2.tar.gz/user-home/.scripts/common-helpers/batch/pmml:dsx-scripted-ml-python2.tar.gz/user-home/.scripts/common-helpers/saas:dsx-scripted-ml-python2.tar.gz/user-home/_global_/python-2.7:'}, 'proxyUser': u'user1', 'driverMemory': '3G'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show spark config used for this session\n",
    "%spark info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Run sample job to verify Watson Studio Virtual Environment Loaded Properly\n",
    "Load Watson Studio packages from the specified image on remote YARN Node Managers, \n",
    "`sklearn`, `numpy`, `pandas`, `keras` \n",
    "Which are not included in default python environments.\n",
    "\n",
    "Each cell which contains `%%spark` as the first line will run against the remote spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Job Ran on YARN NodeManager: \n",
      " shad4.fyre.ibm.com\n",
      "          X         Y  Z\n",
      "0  3.658685  1.811328  2\n",
      "1  0.302049  1.468605  3\n",
      "2  4.339292  4.807158  0\n",
      "3  0.931340  1.003244  3\n",
      "/hadoop/yarn/local/usercache/user1/appcache/application_1533834262477_0022/container_e11_1533834262477_0022_01_000001/dsx-scripted-ml-python2.tar.gz/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend."
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "x = (5*np.random.rand(6**5))\n",
    "y = (5*np.random.rand(6**5))\n",
    "df = pd.DataFrame({'X':x,'Y':y})\n",
    "\n",
    "def determine_z(row):\n",
    "    value = float(np.sin(row['X']) + np.sin(row['Y']))\n",
    "    if value < -1.0:\n",
    "        return 0\n",
    "    if (value >= -1.0) & (value < 0.0):\n",
    "        return 1\n",
    "    if (value >= 0.0) & (value < 1.0):\n",
    "        return 2\n",
    "    if value >= 1.0:\n",
    "        return 3\n",
    "\n",
    "df['Z'] = df.apply(determine_z, axis=1)\n",
    "import socket\n",
    "print(\"Test Job Ran on YARN NodeManager: \\n \" + socket.gethostname() )\n",
    "print(df.head(4))\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['X','Y']],df['Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## 2. Work with data on HDFS\n",
    "\n",
    "#### 2.1 Sending Data to HDFS\n",
    "\n",
    "Notice: These 2 cells are run **without** the %%spark, such that they run locally within Watson Studio.\n",
    "DataSet path (Running as user1):\n",
    "```\n",
    "hdfs:///user/user1/datasets/atlas_higgs.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://asgardian-edge.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1', 'https://becks1.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1', 'https://bendy1.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1', 'https://cdh513edge11.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1', 'https://kabob3.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1', 'https://matzo1.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1', 'https://yccdh5.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1', 'https://zinc1.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1']\n"
     ]
    }
   ],
   "source": [
    "# Show registered WebHDFS Secure URLS which logged in user has access to:\n",
    "import dsx_core_utils\n",
    "dsx_core_utils.list_dsxhi_webhdfs_endpoints();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that that Watson Studio has 8 different HDFS Clusters registered to it. \n",
    "Choose the corresponding endpoint to which to send data to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload success\n"
     ]
    }
   ],
   "source": [
    "dsxlocal_file_location=\"../datasets/atlas_higgs.csv\"\n",
    "dsxhi_upload_hdfs_location=\"/user/user1/datasets/atlas_higgs.csv\"\n",
    "webhdfs_endpoint=\"https://asgardian-edge.fyre.ibm.com:8443/gateway/kanchdsx-310-master-1/webhdfs/v1\"\n",
    "\n",
    "dsx_core_utils.upload_hdfs_file(webhdfs_endpoint, dsxlocal_file_location, dsxhi_upload_hdfs_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Load csv into remote df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "raw_dataset = sqlContext.read.format('com.databricks.spark.csv').options(\n",
    "header='true', inferschema='true').load(\"hdfs:///user/user1/datasets/atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## 3. Run dist-keras on Spark \n",
    "\n",
    "#### 3.1 dist-keras imports\n",
    "One of the Distributed Optimization Algorithms in use for **Distributed Keras on Spark** is **dist-keras**\n",
    "\n",
    "Demo inspired from: https://github.com/cerndb/dist-keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "import numpy as np\n",
    "import time, requests\n",
    "\n",
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "import keras\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.2 Remote Dataset preprocessing and normalization\n",
    "We now preprocess the data from hdfs, concatenating all the features into a single Vector column. More information on Spark MLlib feature transforms can be found here: http://spark.apache.org/docs/latest/ml-guide.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EventId: integer (nullable = true)\n",
      " |-- DER_mass_MMC: double (nullable = true)\n",
      " |-- DER_mass_transverse_met_lep: double (nullable = true)\n",
      " |-- DER_mass_vis: double (nullable = true)\n",
      " |-- DER_pt_h: double (nullable = true)\n",
      " |-- DER_deltaeta_jet_jet: double (nullable = true)\n",
      " |-- DER_mass_jet_jet: double (nullable = true)\n",
      " |-- DER_prodeta_jet_jet: double (nullable = true)\n",
      " |-- DER_deltar_tau_lep: double (nullable = true)\n",
      " |-- DER_pt_tot: double (nullable = true)\n",
      " |-- DER_sum_pt: double (nullable = true)\n",
      " |-- DER_pt_ratio_lep_tau: double (nullable = true)\n",
      " |-- DER_met_phi_centrality: double (nullable = true)\n",
      " |-- DER_lep_eta_centrality: double (nullable = true)\n",
      " |-- PRI_tau_pt: double (nullable = true)\n",
      " |-- PRI_tau_eta: double (nullable = true)\n",
      " |-- PRI_tau_phi: double (nullable = true)\n",
      " |-- PRI_lep_pt: double (nullable = true)\n",
      " |-- PRI_lep_eta: double (nullable = true)\n",
      " |-- PRI_lep_phi: double (nullable = true)\n",
      " |-- PRI_met: double (nullable = true)\n",
      " |-- PRI_met_phi: double (nullable = true)\n",
      " |-- PRI_met_sumet: double (nullable = true)\n",
      " |-- PRI_jet_num: integer (nullable = true)\n",
      " |-- PRI_jet_leading_pt: double (nullable = true)\n",
      " |-- PRI_jet_leading_eta: double (nullable = true)\n",
      " |-- PRI_jet_leading_phi: double (nullable = true)\n",
      " |-- PRI_jet_subleading_pt: double (nullable = true)\n",
      " |-- PRI_jet_subleading_eta: double (nullable = true)\n",
      " |-- PRI_jet_subleading_phi: double (nullable = true)\n",
      " |-- PRI_jet_all_pt: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Label: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "raw_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=DenseVector([138.47, 51.655, 97.827, 27.98, 0.91, 124.711, 2.666, 3.064, 41.928, 197.76, 1.582, 1.396, 0.2, 32.638, 1.017, 0.381, 51.626, 2.273, -2.414, 16.824, -0.277, 258.733, 2.0, 67.435, 2.15, 0.444, 46.062, 1.24, -2.475, 113.497]))]"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "features = raw_dataset.columns\n",
    "features.remove('EventId')\n",
    "features.remove('Weight')\n",
    "features.remove('Label')\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column \"features\" which will contain all the desired features aggregated into a single vector.\n",
    "dataset = vector_assembler.transform(raw_dataset)\n",
    "\n",
    "# Show what happened after applying the vector assembler.\n",
    "# Note: \"features\" column got appended to the end.\n",
    "dataset.select(\"features\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Label=u's', label_index=1.0), Row(Label=u'b', label_index=0.0), Row(Label=u'b', label_index=0.0), Row(Label=u'b', label_index=0.0), Row(Label=u'b', label_index=0.0)]"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_normalized\", withStd=True, withMean=True)\n",
    "standard_scaler_model = standard_scaler.fit(dataset)\n",
    "#mdataset = standard_scaler_model.transform(dataset)\n",
    "# If we look at the dataset, the Label column consists of 2 entries, i.e., b (background), and s (signal).\n",
    "# Our neural network will not be able to handle these characters, so instead, we convert it to an index so we can indicate that output neuron with index 0 is background, and 1 is signal.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label_index\").fit(dataset)\n",
    "dataset = label_indexer.transform(dataset)\n",
    "\n",
    "# Show the result of the label transformation.\n",
    "dataset.select(\"Label\", \"label_index\").take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "nb_classes = 2 # Number of output classes (signal and background)\n",
    "nb_features = len(features)\n",
    "# Shuffle the dataset.\n",
    "dataset = shuffle(dataset)\n",
    "# Note: dist-keras also supports shuffling data in its Trainer implementation by default.\n",
    "# However, since this would require a shuffle for every model we train on the dataset, we perform the shuffle in advance here.\n",
    "\n",
    "# Create a training set and a testset.\n",
    "(training_set, test_set) = dataset.randomSplit([0.6, 0.4])\n",
    "training_set.cache()\n",
    "test_set.cache()\n",
    "\n",
    "# Create a temp view, accessible via spark sql\n",
    "test_set.createOrReplaceTempView(\"test_set_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.2 Model Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                775       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "model = Sequential()\n",
    "hidden_layer_size = 25\n",
    "model.add(Dense(hidden_layer_size, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(hidden_layer_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Set Worker optimizer and loss\n",
    "In order to evaluate the gradient on the model replicas, we have to specify an optimizer and a loss function. dist-keras supports the same optimizers and loss functions as Keras, so we may simply refer to the Keras API documentation for optimizers and objective functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "optimizer = 'adagrad'\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## 4.0 Model Training and Evaluation\n",
    "\n",
    "- Define `evaluate_accuracy` function\n",
    "- Train a model using a `Single Trainer`, `EASGD` methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Define some functions to use in our trainers\n",
    "def evaluate_accuracy(model, test_set):\n",
    "    import time\n",
    "    s = time.time()\n",
    "    # Allocate a Distributed Keras Accuracy evaluator.\n",
    "    evaluator = AccuracyEvaluator(prediction_col=\"prediction_index\", label_col=\"label_index\")\n",
    "    # Clear the prediction column from the testset.\n",
    "    test_set = test_set.select(\"features_normalized\", \"label_index\", \"label\")\n",
    "    # Apply a prediction from a trained model.\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=\"features_normalized\")\n",
    "    test_set = predictor.predict(test_set)\n",
    "    # Allocate an index transformer.\n",
    "    index_transformer = LabelIndexTransformer(output_dim=nb_classes)\n",
    "    # Transform the prediction vector to an indexed label.\n",
    "    test_set = index_transformer.transform(test_set)\n",
    "    # Fetch the score.\n",
    "    score = evaluator.evaluate(test_set)\n",
    "    return score\n",
    "\n",
    "# Number of training epochs to run for each Trainer\n",
    "TRAIN_EPOCHS = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1 Baseline: Single-executor machine training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "trainer = SingleTrainer(keras_model=model, worker_optimizer=optimizer,\n",
    "                        loss=loss, features_col=\"features_normalized\",\n",
    "                        label_col=\"label\", num_epoch=TRAIN_EPOCHS, batch_size=32)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 Asynchronous EASGD\n",
    "EASGD based methods, proposed by Zhang et al., transmit the complete parametrization instead of the gradient. These methods will then \"average\" the difference of the center variable and the backpropagated worker variable. This is used to compute a new master variable, which the worker nodes will utilize during the next iteration of backpropagation.\n",
    "\n",
    "Asynchronous EASGD updates model parameters in an asynchronous fashion - whenever a worker node is done processing its mini-batch after a certain amount of iterations (referred to as the \"communication window\"), then the computed parameter will be communicated with the parameter server. The parameter server will then update the center variable immediately, without waiting for other workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "num_workers = sc.defaultParallelism\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "trainer = AEASGD(keras_model=model, worker_optimizer=optimizer, loss=loss, num_workers=num_workers, \n",
    "                 batch_size=32, features_col=\"features_normalized\", label_col=\"label\", num_epoch=TRAIN_EPOCHS,\n",
    "                 communication_window=32, rho=5.0, learning_rate=0.1)\n",
    "trainer.set_parallelism_factor(num_workers)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Yarn Sample logs from a successful run:\n",
    "```\n",
    "18/05/21 20:58:32 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 910) in 11805 ms on shad2.fyre.ibm.com (executor 2) (1/3)\n",
    "18/05/21 20:58:32 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 911) in 11809 ms on shad1.fyre.ibm.com (executor 3) (2/3)\n",
    "18/05/21 20:58:32 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 912) in 11809 ms on shad4.fyre.ibm.com (executor 5) (3/3)\n",
    "18/05/21 20:58:32 INFO YarnClusterScheduler: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
    "18/05/21 20:58:32 INFO DAGScheduler: ResultStage 30 (collect at build/bdist.linux-x86_64/egg/distkeras/trainers.py:633) finished in 11.812 s\n",
    "18/05/21 20:58:32 INFO DAGScheduler: Job 15 finished: collect at build/bdist.linux-x86_64/egg/distkeras/trainers.py:633, took 11.834807 s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_name': 'Dense', 'config': {'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': u'uniform', 'scale': 1.0, 'seed': None, 'mode': u'fan_avg'}}, 'name': u'dense_1', 'kernel_constraint': None, 'bias_regularizer': None, 'bias_constraint': None, 'dtype': u'float32', 'activation': 'linear', 'trainable': True, 'kernel_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'units': 25, 'batch_input_shape': (None, 30), 'use_bias': True, 'activity_regularizer': None}}, {'class_name': 'Activation', 'config': {'activation': 'relu', 'trainable': True, 'name': u'activation_1'}}, {'class_name': 'Dropout', 'config': {'rate': 0.4, 'noise_shape': None, 'trainable': True, 'seed': None, 'name': u'dropout_1'}}, {'class_name': 'Dense', 'config': {'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': u'uniform', 'scale': 1.0, 'seed': None, 'mode': u'fan_avg'}}, 'name': u'dense_2', 'kernel_constraint': None, 'bias_regularizer': None, 'bias_constraint': None, 'activation': 'linear', 'trainable': True, 'kernel_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'units': 25, 'use_bias': True, 'activity_regularizer': None}}, {'class_name': 'Activation', 'config': {'activation': 'relu', 'trainable': True, 'name': u'activation_2'}}, {'class_name': 'Dense', 'config': {'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': u'uniform', 'scale': 1.0, 'seed': None, 'mode': u'fan_avg'}}, 'name': u'dense_3', 'kernel_constraint': None, 'bias_regularizer': None, 'bias_constraint': None, 'activation': 'linear', 'trainable': True, 'kernel_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'units': 2, 'use_bias': True, 'activity_regularizer': None}}, {'class_name': 'Activation', 'config': {'activation': 'softmax', 'trainable': True, 'name': u'activation_3'}}]"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "trained_model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "When you are done, run `%%spark cleanup` to remove any idle sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
