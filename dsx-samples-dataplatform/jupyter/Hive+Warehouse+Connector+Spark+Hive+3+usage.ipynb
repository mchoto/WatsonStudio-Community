{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Accessing Hive 3.0 via Spark with the Hive Warehouse Connector\n",
    "\n",
    "In HDP 3.0, Hive 3.0 introduced some key changes into how Hive Managed tables can be accessed via Spark. \n",
    "\n",
    "Spark and Hive now use independent catalogs for accessing SparkSQL or Hive tables on the same platform. Accessing Hive from a remote Livy session via SparkSQL will no longer share the same Hive Catalog as Beeline or Hive JDBC Clients. To better understand the Hive 3.0 Changes, lets take a look at the 3 methods for accessing Hive 3.0:\n",
    "\n",
    "**SparkSQL** - Hive thrift\n",
    "- Hive Data scope: Spark Catalog\n",
    "- Native SparkSQL Access to Hive\n",
    "- Spark Driver accesses the metastore, Spark Executors access the data from HDFS in parallel.\n",
    "\n",
    "**HiveWareHouseConnector - Hive JDBC**\n",
    "- Hive Data scope: Hive Catalog\n",
    "- HWC Library needed to execute queries and retrieve results as a DataFrame\n",
    "- Query is submitted to Hive and run on the Hive Engine. \n",
    "\n",
    "**HiveWareHouseConnector - Hive LLAP JDBC**\n",
    "- Hive Data scope: Hive Catalog\n",
    "- Hive Interactive Queries\n",
    "- Query is submitted to Hive LLAP and run on the LLAP Daemons. \n",
    "\n",
    "\n",
    "The [Hive Warehouse Connector](https://github.com/hortonworks-spark/spark-llap/tree/master) is a library to read/write DataFrames and Streaming DataFrames to/from Apache Hive� using LLAP. With Apache Ranger�, this library provides row/column level fine-grained access controls. \n",
    "\n",
    "See the [HDP 3.0.X Documentation](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/integrating-hive/content/hive_hivewarehouseconnector_for_handling_apache_spark_data.html) for further details on these changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> **Prerequisite:** The Hive Warehouse Connector Libraries are distributed on the edge nodes of an HDP Cluster. The DSXHI Administrator can upload them to a shared location on HDFS, or end users may place them on any accessibly path such as their HDFS Home Directories.</div>\n",
    "\n",
    "## Table of Contents\n",
    "This notebook contains these main sections:\n",
    "\n",
    "1. [Using SparkSQL to access the Spark Catalog in Hive](#Spark_Catalog)\n",
    "2. [Using the HWC to access Hive Managed Tables](#HWC_HIVE)\n",
    "3. [Using the HWC to access LLAP Hive Managed Tables](#HWC_HIVE_LLAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Spark_Catalog'></a>\n",
    "# 1. Using SparkSQL To access the Spark Catalog in Hive\n",
    "\n",
    "[SparkSQL](https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html) allows you to use the **SparkSession** which is automatically created in a remote Livy Session, to access the spark catalog within Hive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 Create a Livy Session\n",
    "First, let's import the necessary Python dependencies and see the registered Hadoop Systems available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Hadoop systems: \n",
      "\n",
      "    systemName  LIVYSPARK  LIVYSPARK2                  imageId\n",
      "0  Azeroth-301             livyspark2  dsx-scripted-ml-python2\n",
      "1        zinc1  livyspark  livyspark2  dsx-scripted-ml-python2\n"
     ]
    }
   ],
   "source": [
    "import dsx_core_utils\n",
    "%load_ext sparkmagic.magics\n",
    "\n",
    "# Retrieve a list of registered Hadoop Integration systems.\n",
    "DSXHI_STSEMS = dsx_core_utils.get_dsxhi_info(showSummary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Configure the spark properties** that we will use for interacting with Hive via a remote Livy Session. \n",
    "\n",
    "Additional [Livy properties](https://livy.incubator.apache.org/docs/latest/rest-api.html) can be provided, such as the Yarn Queue, and the driver memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://azeroth-301-edge.fyre.ibm.com:8443/gateway/kanchws-52-nfs-master-1/livy2/v1 \n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "# Set up sparkmagic to connect to the selected registered HI systemName above.\n",
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"1G\",\n",
    " \"numExecutors\": 1,\n",
    "\"executorMemory\":\"1G\"\n",
    "}\n",
    "\n",
    "HI_CONFIG = dsx_core_utils.setup_livy_sparkmagic(\n",
    "  system=\"Azeroth-301\", \n",
    "  livy=\"livyspark2\",\n",
    "  imageId=None,\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load spark magic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>197</td><td>application_1544476065609_0015</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://azeroth-301-ambari.fyre.ibm.com:8088/proxy/application_1544476065609_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://azeroth-301-compute-2.fyre.ibm.com:8042/node/containerlogs/container_e07_1544476065609_0015_01_000002/user1\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'spark_catalog_access'\n",
    "livy_endpoint = HI_CONFIG['LIVY']\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Running sql queries\n",
    "\n",
    "The Spark Catalog can be accessed natively via SparkSQL or SparkMagic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2.1 Show Databases / Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "spark.sql(\"show databases\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "spark.sql(\"use default\").show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2.2 Show tables using Spark Magic\n",
    "**Note** Accessing the Spark Catalog requires [Hive Acid Transactions](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.0/managing-hive/content/hive_acid_operations.html) to be enabled. \n",
    "\n",
    "`Tip: Spark Magic may return an Encoding stacktrace on the first run of this cell. Re-running the cell allows it to render properly.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>spark_catalog_tabletest</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>default</td>\n",
       "      <td>t1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database                tableName  isTemporary\n",
       "0  default  spark_catalog_tabletest        False\n",
       "1  default                       t1        False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql -s $session_name\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql -s $session_name \n",
    "insert into spark_catalog_tabletest values(\"Other\",\"Two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.2.3 Store query results in a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(col1,StringType,true),StructField(cold2,StringType,true)))"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "REMOTE_DF = spark.sql(\"select * from spark_catalog_tabletest\")\n",
    "\n",
    "REMOTE_DF.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.2.4 Retrieve query results to jupyter and visualize\n",
    "\n",
    "Vizualiztion libraries require the Data to be present within Jupyter's Kernel (Running on Watson Studio). \n",
    "\n",
    "This means that a Spark DataFrame must be retrieved from the running session, into Jupyter to be able to apply a visualiztion library on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -o WSL_DF -n 5\n",
    "WSL_DF = spark.sql(\"select * from spark_catalog_tabletest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note** `WSL_DF` is now a local dataframe within Jupyter. The following cell runs locally within jupyter, as it does **not** include `%%spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.11</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Warning: You are not running the latest version of PixieDust. Current is 1.1.11, Latest is 1.1.14</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>Please copy and run the following command in a new cell to upgrade: <span style=\"background-color:#ececec;font-family:monospace;padding:0 5px\">!pip install --user --upgrade pixiedust</span></div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Please restart kernel after upgrading.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>\n",
       "        <div class=\"pd_save is-viewer-good\" style=\"padding-right:10px;text-align: center;line-height:initial !important;font-size: xx-large;font-weight: 500;color: coral;\">\n",
       "            \n",
       "        </div>\n",
       "    <div id=\"chartFiguref9010d9d\" class=\"pd_save is-viewer-good\" style=\"overflow-x:auto\">\n",
       "            <style type=\"text/css\" class=\"pd_save\">\n",
       "    .df-table-wrapper .panel-heading {\n",
       "      border-radius: 0;\n",
       "      padding: 0px;\n",
       "    }\n",
       "    .df-table-wrapper .panel-heading:hover {\n",
       "      border-color: #008571;\n",
       "    }\n",
       "    .df-table-wrapper .panel-title a {\n",
       "      background-color: #f9f9fb;\n",
       "      color: #333333;\n",
       "      display: block;\n",
       "      outline: none;\n",
       "      padding: 10px 15px;\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .df-table-wrapper .panel-title a:hover {\n",
       "      background-color: #337ab7;\n",
       "      border-color: #2e6da4;\n",
       "      color: #ffffff;\n",
       "      display: block;\n",
       "      padding: 10px 15px;\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .df-table-wrapper {\n",
       "      font-size: small;\n",
       "      font-weight: 300;\n",
       "      letter-spacing: 0.5px;\n",
       "      line-height: normal;\n",
       "      height: inherit;\n",
       "      overflow: auto;\n",
       "    }\n",
       "    .df-table-search {\n",
       "      margin: 0 0 20px 0;\n",
       "    }\n",
       "    .df-table-search-count {\n",
       "      display: inline-block;\n",
       "      margin: 0 0 20px 0;\n",
       "    }\n",
       "    .df-table-container {\n",
       "      max-height: 50vh;\n",
       "      max-width: 100%;\n",
       "      overflow-x: auto;\n",
       "      position: relative;\n",
       "    }\n",
       "    .df-table-wrapper table {\n",
       "      border: 0 none #ffffff;\n",
       "      border-collapse: collapse;\n",
       "      margin: 0;\n",
       "      min-width: 100%;\n",
       "      padding: 0;\n",
       "      table-layout: fixed;\n",
       "      height: inherit;\n",
       "      overflow: auto;\n",
       "    }\n",
       "    .df-table-wrapper tr.hidden {\n",
       "      display: none;\n",
       "    }\n",
       "    .df-table-wrapper tr:nth-child(even) {\n",
       "      background-color: #f9f9fb;\n",
       "    }\n",
       "    .df-table-wrapper tr.even {\n",
       "      background-color: #f9f9fb;\n",
       "    }\n",
       "    .df-table-wrapper tr.odd {\n",
       "      background-color: #ffffff;\n",
       "    }\n",
       "    .df-table-wrapper td + td {\n",
       "      border-left: 1px solid #e0e0e0;\n",
       "    }\n",
       "  \n",
       "    .df-table-wrapper thead,\n",
       "    .fixed-header {\n",
       "      font-weight: 600;\n",
       "    }\n",
       "    .df-table-wrapper tr,\n",
       "    .fixed-row {\n",
       "      border: 0 none #ffffff;\n",
       "      margin: 0;\n",
       "      padding: 0;\n",
       "    }\n",
       "    .df-table-wrapper th,\n",
       "    .df-table-wrapper td,\n",
       "    .fixed-cell {\n",
       "      border: 0 none #ffffff;\n",
       "      margin: 0;\n",
       "      min-width: 50px;\n",
       "      padding: 5px 20px 5px 10px;\n",
       "      text-align: left;\n",
       "      word-wrap: break-word;\n",
       "    }\n",
       "    .df-table-wrapper th {\n",
       "      padding-bottom: 0;\n",
       "      padding-top: 0;\n",
       "    }\n",
       "    .df-table-wrapper th div {\n",
       "      max-height: 1px;\n",
       "      visibility: hidden;\n",
       "    }\n",
       "  \n",
       "    .df-schema-field {\n",
       "      margin-left: 10px;\n",
       "    }\n",
       "  \n",
       "    .fixed-header-container {\n",
       "      overflow: hidden;\n",
       "      position: relative;\n",
       "    }\n",
       "    .fixed-header {\n",
       "      border-bottom: 2px solid #000;\n",
       "      display: table;\n",
       "      position: relative;\n",
       "    }\n",
       "    .fixed-row {\n",
       "      display: table-row;\n",
       "    }\n",
       "    .fixed-cell {\n",
       "      display: table-cell;\n",
       "    }\n",
       "  </style>\n",
       "  \n",
       "  \n",
       "  <div class=\"df-table-wrapper df-table-wrapper-f9010d9d panel-group pd_save\">\n",
       "    <!-- dataframe schema -->\n",
       "    \n",
       "    <div class=\"panel panel-default\">\n",
       "      <div class=\"panel-heading\">\n",
       "        <h4 class=\"panel-title\" style=\"margin: 0px;\">\n",
       "          <a data-toggle=\"collapse\" href=\"#df-schema-f9010d9d\" data-parent=\"#df-table-wrapper-f9010d9d\">Schema</a>\n",
       "        </h4>\n",
       "      </div>\n",
       "      <div id=\"df-schema-f9010d9d\" class=\"panel-collapse collapse\">\n",
       "        <div class=\"panel-body\" style=\"font-family: monospace;\">\n",
       "          <div class=\"df-schema-fields\">\n",
       "            <div>Field types:</div>\n",
       "            \n",
       "              <div class=\"df-schema-field\"><strong>col1: </strong> object</div>\n",
       "            \n",
       "              <div class=\"df-schema-field\"><strong>cold2: </strong> object</div>\n",
       "            \n",
       "          </div>\n",
       "        </div>\n",
       "      </div>\n",
       "    </div>\n",
       "    \n",
       "    <!-- dataframe table -->\n",
       "    <div class=\"panel panel-default\">\n",
       "      \n",
       "      <div class=\"panel-heading\">\n",
       "        <h4 class=\"panel-title\" style=\"margin: 0px;\">\n",
       "          <a data-toggle=\"collapse\" href=\"#df-table-f9010d9d\" data-parent=\"#df-table-wrapper-f9010d9d\"> Table</a>\n",
       "        </h4>\n",
       "      </div>\n",
       "      \n",
       "      <div id=\"df-table-f9010d9d\" class=\"panel-collapse collapse in\">\n",
       "        <div class=\"panel-body\">\n",
       "          \n",
       "          <input class=\"df-table-search form-control input-sm\" placeholder=\"Search table\" type=\"text\">\n",
       "          \n",
       "          <div>\n",
       "            \n",
       "            <span class=\"df-table-search-count\">Showing 5 of 5 rows</span>\n",
       "            \n",
       "          </div>\n",
       "          <!-- fixed header for when dataframe table scrolls -->\n",
       "          <div class=\"fixed-header-container\">\n",
       "            <div class=\"fixed-header\" style=\"width: 1419px;\">\n",
       "              <div class=\"fixed-row\">\n",
       "                \n",
       "                <div class=\"fixed-cell\" style=\"width: 701px;\">col1</div>\n",
       "                \n",
       "                <div class=\"fixed-cell\" style=\"width: 719px;\">cold2</div>\n",
       "                \n",
       "              </div>\n",
       "            </div>\n",
       "          </div>\n",
       "          <div class=\"df-table-container\">\n",
       "            <table class=\"df-table\">\n",
       "              <thead>\n",
       "                <tr>\n",
       "                  \n",
       "                  <th><div>col1</div></th>\n",
       "                  \n",
       "                  <th><div>cold2</div></th>\n",
       "                  \n",
       "                </tr>\n",
       "              </thead>\n",
       "              <tbody>\n",
       "                \n",
       "                <tr>\n",
       "                  \n",
       "                  <td>Other</td>\n",
       "                  \n",
       "                  <td>Two</td>\n",
       "                  \n",
       "                </tr>\n",
       "                \n",
       "                <tr>\n",
       "                  \n",
       "                  <td>Other</td>\n",
       "                  \n",
       "                  <td>Two</td>\n",
       "                  \n",
       "                </tr>\n",
       "                \n",
       "                <tr>\n",
       "                  \n",
       "                  <td>Other</td>\n",
       "                  \n",
       "                  <td>Two</td>\n",
       "                  \n",
       "                </tr>\n",
       "                \n",
       "                <tr>\n",
       "                  \n",
       "                  <td>Other</td>\n",
       "                  \n",
       "                  <td>Two</td>\n",
       "                  \n",
       "                </tr>\n",
       "                \n",
       "                <tr>\n",
       "                  \n",
       "                  <td>Other</td>\n",
       "                  \n",
       "                  <td>Two</td>\n",
       "                  \n",
       "                </tr>\n",
       "                \n",
       "              </tbody>\n",
       "            </table>\n",
       "          </div>\n",
       "        </div>\n",
       "      </div>\n",
       "    </div>\n",
       "  </div>\n",
       "  \n",
       "  <script class=\"pd_save\">\n",
       "    $(function() {\n",
       "      var tableWrapper = $('.df-table-wrapper-f9010d9d');\n",
       "      var fixedHeader = $('.fixed-header', tableWrapper);\n",
       "      var tableContainer = $('.df-table-container', tableWrapper);\n",
       "      var table = $('.df-table', tableContainer);\n",
       "      var rows = $('tbody > tr', table);\n",
       "      var total = 5;\n",
       "  \n",
       "      fixedHeader\n",
       "        .css('width', table.width())\n",
       "        .find('.fixed-cell')\n",
       "        .each(function(i, e) {\n",
       "          $(this).css('width', $('.df-table-wrapper-f9010d9d th:nth-child(' + (i+1) + ')').css('width'));\n",
       "        });\n",
       "  \n",
       "      tableContainer.scroll(function() {\n",
       "        fixedHeader.css({ left: table.position().left });\n",
       "      });\n",
       "  \n",
       "      rows.on(\"click\", function(e){\n",
       "          var txt = e.delegateTarget.innerText;\n",
       "          var splits = txt.split(\"\\t\");\n",
       "          var len = splits.length;\n",
       "          var hdrs = $(fixedHeader).find(\".fixed-cell\");\n",
       "          // Add all cells in the selected row as a map to be consumed by the target as needed\n",
       "          var payload = {type:\"select\", targetDivId: \"\" };\n",
       "          for (var i = 0; i < len; i++) {\n",
       "            payload[hdrs[i].innerHTML] = splits[i];\n",
       "          }\n",
       "  \n",
       "          //simple selection highlighting, client adds \"selected\" class\n",
       "          $(this).addClass(\"selected\").siblings().removeClass(\"selected\");\n",
       "          $(document).trigger('pd_event', payload);\n",
       "      });\n",
       "  \n",
       "      $('.df-table-search', tableWrapper).keyup(function() {\n",
       "        var val = '^(?=.*\\\\b' + $.trim($(this).val()).split(/\\s+/).join('\\\\b)(?=.*\\\\b') + ').*$';\n",
       "        var reg = RegExp(val, 'i');\n",
       "        var index = 0;\n",
       "        \n",
       "        rows.each(function(i, e) {\n",
       "          if (!reg.test($(this).text().replace(/\\s+/g, ' '))) {\n",
       "            $(this).attr('class', 'hidden');\n",
       "          }\n",
       "          else {\n",
       "            $(this).attr('class', (++index % 2 == 0 ? 'even' : 'odd'));\n",
       "          }\n",
       "        });\n",
       "        $('.df-table-search-count', tableWrapper).html('Showing ' + index + ' of ' + total + ' rows');\n",
       "      });\n",
       "    });\n",
       "  \n",
       "    $(\".df-table-wrapper td:contains('http://')\").each(function(){var tc = this.textContent; $(this).wrapInner(\"<a target='_blank' href='\" + tc + \"'></a>\");});\n",
       "    $(\".df-table-wrapper td:contains('https://')\").each(function(){var tc = this.textContent; $(this).wrapInner(\"<a target='_blank' href='\" + tc + \"'></a>\");});\n",
       "  </script>\n",
       "  \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(WSL_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Important** Close the existing Livy Connection before proceeding to Part 2, as we'll be creating a **new** livy session for each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%spark delete -s $session_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='HWC_HIVE'></a>\n",
    "---\n",
    "# 2. Using the HWC to access Hive Managed Tables\n",
    "\n",
    "The Hive Warehouse Connector allows Spark to access to the follow operations from Hive:\n",
    "\n",
    "**Supported Catalog Operations**\n",
    "- Set the current database for unqualified Hive table references<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`hivecon.setDatabase(<database>)`\n",
    "\n",
    "- Execute a catalog operation and return a DataFrame<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`hivecon.execute(\"describe extended web_sales\").show(100)`\n",
    "\n",
    "- Show databases<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`hivecon.showDatabases().show(100)`\n",
    "\n",
    "- Show tables for the current database<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`hivecon.showTables().show(100)`\n",
    "\n",
    "- Describe a table<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`hivecon.describeTable(<table_name>).show(100)`\n",
    "\n",
    "- Create a database<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`hivecon.createDatabase(<database_name>,<ifNotExists>)`\n",
    "\n",
    "**Supported Read Operations**\n",
    "- Execute a Hive SELECT query and return a DataFrame.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`DF1 = hivecon.executeQuery(\"select * from web_sales\")`\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Create a new Livy Session\n",
    "Create a new Livy Connection, this time passing in the Spark Configuration **jars** and **pyFiles**, indicating the HDFS Location of the Hive Warehouse Connector libraries. \n",
    "\n",
    "**Recommended** The `conf` settings can be applied in Ambari as spark2-defaults for ALL Spark2-client dependent applications. This may be ideal for some situations, as it will make it easier for Data Scientists when configuring their Spark Session Properties. \n",
    "\n",
    "Required Spark Conf For Kerberized Clusters:\n",
    "\n",
    "- spark.sql.hive.hiveserver2.jdbc.url\n",
    "- spark.datasource.hive.warehouse.metastoreUri\n",
    "- spark.datasource.hive.warehouse.load.staging.dir\n",
    "- spark.hadoop.hive.llap.daemon.service.hosts\n",
    "- spark.hadoop.hive.zookeeper.quorum\n",
    "- spark.sql.hive.hiveserver2.jdbc.url.principal\n",
    "\n",
    "For a detailed explanation on **where** to obtain the `conf` properties from Ambari, see: https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/integrating-hive/content/hive_configure_a_spark_hive_connection.html\n",
    "\n",
    "**Notice the `.jdbc.url` endpoints:**\n",
    "- `zooKeeperNamespace=hiveserver2`  || Allows you to Connect to Hive Managed Tables using HWC: \n",
    "- `zooKeeperNamespace=hiveserver2-interactive`  || Allows you to Connect to LLAP-Hive Managed Tables using HWC: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> **IMPORTANT:** If working in a Kerberized Environment, the Hive Warehouse Connector Libraries used in `jars` and `pyFiles` MUST be read from the local filesystem, e.g. `file:///usr/hdp/current` and **not** from HDFS. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://azeroth-301-edge.fyre.ibm.com:8443/gateway/kanchws-52-nfs-master-1/livy2/v1 \n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"1G\",\n",
    " \"numExecutors\": 1,\n",
    " \"jars\": [\"file:///usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.0.1.0-187.jar\"],\n",
    " \"pyFiles\": [\"file:///usr/hdp/current/hive_warehouse_connector/pyspark_hwc-1.0.0.3.0.1.0-187.zip\"],\n",
    " \"conf\": { \"spark.sql.hive.hiveserver2.jdbc.url\": \n",
    "          \"jdbc:hive2://azeroth-301-master-1.fyre.ibm.com:2181,azeroth-301-ambari.fyre.ibm.com:2181,azeroth-301-master-2.fyre.ibm.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\",\n",
    "           \"spark.datasource.hive.warehouse.metastoreUri\":\"thrift://azeroth-301-master-1.fyre.ibm.com:9083\",\n",
    "           \"spark.datasource.hive.warehouse.load.staging.dir\":\"/tmp\",\n",
    "           \"spark.hadoop.hive.llap.daemon.service.hosts\":\"@llap0\",\n",
    "           \"spark.hadoop.hive.zookeeper.quorum\":\"azeroth-301-master-2.fyre.ibm.com:2181,azeroth-301-ambari.fyre.ibm.com:2181,azeroth-301-master-1.fyre.ibm.com:2181\",\n",
    "           \"spark.sql.hive.hiveserver2.jdbc.url.principal\":\"hive/_HOST@FYRE.IBM.COM\"\n",
    "         }}\n",
    "HI_CONFIG = dsx_core_utils.setup_livy_sparkmagic(\n",
    "  system=\"Azeroth-301\", \n",
    "  livy=\"livyspark2\",\n",
    "  imageId=None,\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load spark magic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>198</td><td>application_1544476065609_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://azeroth-301-ambari.fyre.ibm.com:8088/proxy/application_1544476065609_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://azeroth-301-compute-2.fyre.ibm.com:8042/node/containerlogs/container_e07_1544476065609_0016_01_000002/user1\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'hive_catalog_access'\n",
    "livy_endpoint = HI_CONFIG['LIVY']\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.1.1 Loading the Hive Warehouse connector\n",
    "See [HDP 3.0.1](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/integrating-hive/content/hive_hivewarehousesession_api_operations.html) Docs for Extended usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name \n",
    "from pyspark_llap import HiveWarehouseSession\n",
    "hivecon = HiveWarehouseSession.session(spark).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Accessing Hive Managed Tables via HWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.2.1 Show databases / tables\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|database_name|\n",
      "+-------------+\n",
      "|      default|\n",
      "+-------------+"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name \n",
    "hivecon.showDatabases().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Notice - The Spark Catalog tables and Hive Catalog tables returned will differ: </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Hive Catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          tab_name|\n",
      "+------------------+\n",
      "|       atlas_higgs|\n",
      "| hive_catalog_test|\n",
      "|hive_catalog_test2|\n",
      "|hive_catalog_test3|\n",
      "|         web_sales|\n",
      "+------------------+"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name \n",
    "hivecon.showTables().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark Catalog\n",
    "- **Note** SparkMagic (`%%spark -c sql`) is **not** supported for HWC, as it relies on the Spark Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>spark_catalog_tabletest</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>default</td>\n",
       "      <td>t1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database                tableName  isTemporary\n",
       "0  default  spark_catalog_tabletest        False\n",
       "1  default                       t1        False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -s $session_name -c sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.2.2 Create test table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name \n",
    "hivecon.createTable(\"hive_catalog_test3\").ifNotExists().column(\"sold_time_sk\", \"bigint\"\n",
    "                                             ).column(\"ws_ship_date_sk\", \"bigint\"\n",
    "                                             ).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          tab_name|\n",
      "+------------------+\n",
      "|       atlas_higgs|\n",
      "| hive_catalog_test|\n",
      "|hive_catalog_test2|\n",
      "|hive_catalog_test3|\n",
      "|         web_sales|\n",
      "+------------------+"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "hivecon.showTables().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='HWC_HIVE_LLAP'></a>\n",
    "---\n",
    "\n",
    "## 3. Using the HWC to access Hive Managed Interactive Tables ( LLAP)\n",
    "\n",
    "Connecting to LLAP Requires the same set of properties from section 3, with an updated `spark.sql.hive.hiveserver2.jdbc.url`.\n",
    "\n",
    "The JDBC URL can be found from Ambari > Hive Summary, under  `HIVESERVER2 INTERACTIVE JDBC URL`. \n",
    "\n",
    "Note, for this example the following have been configured in Ambari under Spark > Configs > Advanced > Custom Spark-defaults.\n",
    "- `spark.datasource.hive.warehouse.metastoreUri`\n",
    "- `spark.datasource.hive.warehouse.load.staging.dir`\n",
    "- `spark.hadoop.hive.llap.daemon.service.hosts`\n",
    "- `spark.hadoop.hive.zookeeper.quorum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://azeroth-301-edge.fyre.ibm.com:8443/gateway/kanchws-52-nfs-master-1/livy2/v1 \n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"1G\",\n",
    " \"numExecutors\": 1,\n",
    " \"jars\": [\"file:///usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.0.1.0-187.jar\"],\n",
    " \"pyFiles\": [\"file:///usr/hdp/current/hive_warehouse_connector/pyspark_hwc-1.0.0.3.0.1.0-187.zip\"],\n",
    " \"conf\": { \"spark.sql.hive.hiveserver2.jdbc.url\": \n",
    "          \"jdbc:hive2://azeroth-301-master-1.fyre.ibm.com:2181,azeroth-301-ambari.fyre.ibm.com:2181,azeroth-301-master-2.fyre.ibm.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-interactive\"\n",
    "         }}\n",
    "HI_CONFIG = dsx_core_utils.setup_livy_sparkmagic(\n",
    "  system=\"Azeroth-301\", \n",
    "  livy=\"livyspark2\",\n",
    "  imageId=None,\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load spark magic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1 Start a new Livy Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>199</td><td>application_1544476065609_0017</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://azeroth-301-ambari.fyre.ibm.com:8088/proxy/application_1544476065609_0017/\">Link</a></td><td><a target=\"_blank\" href=\"http://azeroth-301-compute-2.fyre.ibm.com:8042/node/containerlogs/container_e07_1544476065609_0017_01_000001/user1\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'llap_access'\n",
    "livy_endpoint = HI_CONFIG['LIVY']\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name \n",
    "from pyspark_llap import HiveWarehouseSession\n",
    "hivecon = HiveWarehouseSession.session(spark).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2 Accessing Hive Managed Tables via HWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|database_name|\n",
      "+-------------+\n",
      "|      default|\n",
      "+-------------+"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name \n",
    "hivecon.showDatabases().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The **hive_catalog_test** table created in section 2.2 is visible for both LLAP and Non LLAP Connections via HWC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          tab_name|\n",
      "+------------------+\n",
      "|       atlas_higgs|\n",
      "| hive_catalog_test|\n",
      "|hive_catalog_test2|\n",
      "|hive_catalog_test3|\n",
      "|         web_sales|\n",
      "+------------------+"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name \n",
    "hivecon.showTables().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name  \n",
    "DF1 = hivecon.executeQuery(\"select * from hive_catalog_test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, sold_time_sk: string, ws_ship_date_sk: string]"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name \n",
    "DF1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
