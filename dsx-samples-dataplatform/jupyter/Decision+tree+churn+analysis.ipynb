{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create a Decision Tree model for Churn Analysis\n",
    "Spark comes with multiple popular machine learning algorithms. In this tutorial, we are looking at decision trees to determine the churn of telco customers. Decision trees are easier to understand in concept than many other machine learning algorithms since many people have been exposed to decisions such as: if this, then that, else another thing. We can easily understand how decisions are made this way through divide and conquer.\n",
    "\n",
    "Decision trees are more than evaluating an attribute value and decide what to do next. This algorithm looks at the input data and decides how significant each attribute is, how it defines grouping between multiple records. Once this analysis is done it can decide which attribute nd which value range can lead to a decision. \n",
    "\n",
    "We start by getting a Spark session and reading the data into a DataFrame. The `data_df.show(3)` forces the instantiation of the data and provides a formatted view of it. Other methods could have been used, as seen in lab 1, such as the `take` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "| ID|CHURN|Gender|Status|Children|Est Income|Car Owner|      Age|LongDistance|International| Local|Dropped|Paymethod|LocalBilltype|LongDistanceBilltype| Usage|RatePlan|\n",
      "+---+-----+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "|  1|    T|     F|     S|     1.0|   38000.0|        N|24.393333|       23.56|          0.0|206.08|    0.0|       CC|       Budget|      Intnl_discount|229.64|     3.0|\n",
      "|  6|    F|     M|     M|     2.0|   29616.0|        N|49.426667|       29.78|          0.0|  45.5|    0.0|       CH|    FreeLocal|            Standard| 75.29|     2.0|\n",
      "|  8|    F|     M|     M|     0.0|   19732.8|        N|50.673333|       24.81|          0.0| 22.44|    0.0|       CC|    FreeLocal|            Standard| 47.25|     3.0|\n",
      "+---+-----+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Add asset from file system\n",
    "data_df = spark.read.csv('../datasets/customer_churn.csv', header='true', inferSchema = 'true')\n",
    "data_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- CHURN: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Children: double (nullable = true)\n",
      " |-- Est Income: double (nullable = true)\n",
      " |-- Car Owner: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- LongDistance: double (nullable = true)\n",
      " |-- International: double (nullable = true)\n",
      " |-- Local: double (nullable = true)\n",
      " |-- Dropped: double (nullable = true)\n",
      " |-- Paymethod: string (nullable = true)\n",
      " |-- LocalBilltype: string (nullable = true)\n",
      " |-- LongDistanceBilltype: string (nullable = true)\n",
      " |-- Usage: double (nullable = true)\n",
      " |-- RatePlan: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the schema. See that data types were inferred\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare the data before creating the model\n",
    "Some columns have discrete string values: Gender, Status, Car Owner, and so on. <br/>\n",
    "We use a __`StringIndexer`__ to convert the values to numbers.\n",
    "\n",
    "We also convert the 17 columns into a vector so all \"features\" are in one column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the indexers\n",
    "Converting the discrete values to index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline, Model\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "churn_indexer = StringIndexer(inputCol=\"CHURN\", outputCol=\"label\").fit(data_df)\n",
    "gender_indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"IXGender\")\n",
    "status_indexer = StringIndexer(inputCol=\"Status\", outputCol=\"IXStatus\")\n",
    "car_indexer = StringIndexer(inputCol=\"Car Owner\", outputCol=\"IXCarOwner\")\n",
    "pay_indexer = StringIndexer(inputCol=\"Paymethod\", outputCol=\"IXPaymethod\")\n",
    "localbill_indexer = StringIndexer(inputCol=\"LocalBilltype\", outputCol=\"IXLocalBilltype\")\n",
    "long_indexer = StringIndexer(inputCol=\"LongDistanceBilltype\", outputCol=\"IXLongDistanceBilltype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the conversion of columns to vector\n",
    "Note the following statement:<br/>\n",
    "`dt = DecisionTreeClassifier(maxDepth=4, labelCol=\"label\")`\n",
    "\n",
    "In this statement we limit the depth of the tree to 4. This is an arbitrary value that could be changed. It limits the granularity of the decision and can help avlid what is called **overfitting**. This is an important concept that you may want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(inputCols=[\"ID\", \"IXGender\", \"IXStatus\", \"Children\", \"Est Income\", \"IXCarOwner\", \"Age\", \n",
    "               \"LongDistance\", \"International\", \"Local\", \"Dropped\", \"IXPaymethod\", \"IXLocalBilltype\", \n",
    "               \"IXLongDistanceBilltype\", \"Usage\", \"RatePlan\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth=4, labelCol=\"label\")\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=churn_indexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the pipeline that converts the data\n",
    "A pipeline is the set of steps that were defined earlier that are put together in a series of processing steps. We then apply the pipeline to data to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[churn_indexer, gender_indexer, status_indexer, car_indexer, pay_indexer, \n",
    "                               localbill_indexer, long_indexer, vectorAssembler_features, dt, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the model\n",
    "Note that we split the input data into training data to create the model and testing data to evaluate its accuracy. In many cases, it is split into three groups with a validation group that can be used to see if the mode is degrading over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Randomly select records and get to 80% of the data in training_df and 20% in testing_df\n",
    "(training_df, testing_df) = data_df.randomSplit([0.80, 0.20], 123)\n",
    "model = pipeline_rf.fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test the model accuracy\n",
    "The model fits the training data. We can tests the accuracy of the model on data that was not part of the model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.85901\n",
      "Test Error = 0.14099\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(testing_df)\n",
    "evaluatorRF = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Save and Load Model\n",
    "A model can be crerated in one Notebook or through the Watson Studio \"Model\" creation, and reused in another notebook. It is even possible to publish it and use it through a REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from dsx_ml.ml import save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example of saving a model\n",
    "Assumes the model we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Telco Churn Prediction Model 02\"\n",
    "saved_model_output = save(name = model_name, model = model, algorithm_type = 'Classification', test_data = testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('/user-home/{}/DSX_Projects/{}/models/Telco Churn Prediction Model 02/metadata.json'.format(os.environ['DSX_USER_ID'],os.environ['DSX_PROJECT_NAME'])) as infile:\n",
    "    metadata_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type: PipelineModel\n",
      "Feature(s):\n",
      "    Status\n",
      "    CHURN\n",
      "    Paymethod\n",
      "    Gender\n",
      "    Age\n",
      "    RatePlan\n",
      "    Car Owner\n",
      "    Children\n",
      "    Usage\n",
      "    LongDistance\n",
      "    Dropped\n",
      "    LongDistanceBilltype\n",
      "    International\n",
      "    Est Income\n",
      "    Local\n",
      "    ID\n",
      "    LocalBilltype\n",
      "Latest Model Version: 3\n",
      "Label(s):\n",
      "    CHURN\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Type: {}\".format(metadata_dict['algorithm']))\n",
    "\n",
    "print(\"Feature(s):\")\n",
    "for feature in metadata_dict['features']:\n",
    "    print('    '+feature['name'])\n",
    "\n",
    "print(\"Latest Model Version: {}\".format(metadata_dict['latestModelVersion']))\n",
    "print(\"Label(s):\")\n",
    "for label in metadata_dict['labelColumns']:\n",
    "    print('    '+label['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Score the model\n",
    "In Watson Studio, a test API endpoint for scoring is created upon saving the model.\n",
    "\n",
    "Now, you can send (POST) new scoring records (new data) for which you would like to get predictions. To do that, execute the following sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dsxl-api/v3/project/score/Python27/spark-2.0/Project%201/Telco%20Churn%20Prediction%20Model%2002/3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "header_online = {'Content-Type': 'application/json', 'Authorization': os.environ['DSX_TOKEN']}\n",
    "\n",
    "print(saved_model_output['scoring_endpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "payload_scoring = [{\"ID\":23, \"Gender\":\"M\", \"Status\":\"S\", \"Children\":1, \"Est Income\":50000, \"Car Owner\":\"N\", \"Age\":30, \"LongDistance\":23.45, \"International\":0, \"Local\":200, \"Dropped\":0, \"Paymethod\":\"CC\", \"LocalBilltype\":\"Budget\", \"LongDistanceBilltype\":\"Standard\", \"Usage\":200, \"RatePlan\":2}]\n",
    "response_scoring = requests.post(saved_model_output['scoring_endpoint'], json=payload_scoring, headers=header_online)\n",
    "\n",
    "response_dict = json.loads(response_scoring.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. F\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "for response in response_dict['object']['output']['predictions']:\n",
    "    print(\"{}. {}\".format(n,response))\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
