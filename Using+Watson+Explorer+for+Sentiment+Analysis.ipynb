{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Using Watson Explorer for Sentiment Analysis\n",
    "\n",
    "This notebook shows you how to use Watson Explorer from Watson Studio to analyze unstructured content to perform sentiment analysis.\n",
    "This requires Watson Explorer add-on package installed prior to executing this notebook on the cluster.\n",
    "\n",
    "The data sets for this notebook are real world data from the Airbnb guest reviews of their lodging experience. You'll learn the basic steps to analyze the unstructured data through Watson Explorer and narrow the focus on your unstructured data quickly. In the process you will use Python and WEX, and create sentiment heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Table of contents\n",
    "This notebook contains the followiong main sections:\n",
    "\n",
    "- [Prepare the environment](#prepare_environment)\n",
    "- [Load data](#load_data)\n",
    "- [Create a Dataset](#create_dataset)\n",
    "- [Add documents to the Dataset](#add_documents)\n",
    "- [Locate Part-of-Speech and Sentiment enrichments](#locate_enrichment)\n",
    "- [Create a collection](#create_collection)\n",
    "- [Analyze positive targets](#analyze_positive_target)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='prepare_environment'></a>\n",
    "## Prepare the environment\n",
    "To access Watson Explorer from the notebook, import Watson Explorer Python libraries and specify the endpoint and access information to your Watson Explorer deployment with the following code sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import ibmwex\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a client instance\n",
    "configuration = ibmwex.Configuration()\n",
    "api_client = ibmwex.ApiClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='load_data'></a>\n",
    "## Load data\n",
    "The Airbnb reviews for Boston data set can be obtained from [Inside Airbnb](http://insideairbnb.com/about.html).\n",
    "\n",
    "To get your access key link and load this data set:\n",
    "1. Find the data set and obtain an access key link:\n",
    "    1. On the Watson Studio home page, search for \"airbnb\".\n",
    "    1. Click the card with the title [Airbnb Data for Analytics: Boston Reviews](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/107ab470f90be9a4815791d8ec8c2a9e).\n",
    "    1. Click the link button.\n",
    "    1. Hover above the link button next to the access key to display the link.\n",
    "    1. Double-click the link to select it, copy the link, and click **Close**.\n",
    "    1. Insert the access key link and run the cell to load the data:\n",
    "    1. In the cell below, replace the **LINK-TO-DATA** string in the `read_csv()` method with the link.\n",
    "    1. To verify functionality, and see how to load data, run the following cell to import the pandas and numpy libraries, rename and load five columns of the data into the DataFrame, and show the first five rows of guest review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File LINK-TO-DATA does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9196d3cf8f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Guest reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LINK-TO-DATA\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'reviewer_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'comments'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'host_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'reviewer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'comments'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'host'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File LINK-TO-DATA does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Guest reviews\n",
    "reviews = pd.read_csv(\"LINK-TO-DATA\", usecols=['id','date','reviewer_name','comments','host_name'])\n",
    "reviews.columns=['id','date','reviewer','comments','host']\n",
    "reviews[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='create_dataset'></a>\n",
    "## Create a Dataset\n",
    "A Dataset is a container where documents are stored for analysis. You can use crawlers and importers to import your data into the Dataset. Once your data is loaded into the Dataset, you can associate it to a Collection and then begin analysis.\n",
    "\n",
    "The following python code creates a Dataset named 'Airbnb Data : Boston Reviews' with the four fields of date, reviewer, host, and comments:\n",
    "* `date` : Review date\n",
    "* `reviewer` : Reviewer name\n",
    "* `host` : Host name\n",
    "* `comments` : Review comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a Dataset\n",
    "api_datasets = ibmwex.DatasetApi(api_client)\n",
    "dataset = ibmwex.Dataset()\n",
    "dataset.name = 'Airbnb Data : Boston Reviews'\n",
    "dataset.fields = [ibmwex.Field(name='date', type='Date'), ibmwex.Field(name='reviewer', type='String'), ibmwex.Field(name='comments', type='String'), ibmwex.Field(name='host', type='String')]\n",
    "dataset = api_datasets.create(dataset)\n",
    "\n",
    "print(dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='add_documents'></a>\n",
    "## Add documents to the Dataset\n",
    "In this step, you will use REST API to push data directly into the Dataset. For illustrative purposes, the following python code adds the first 1000 documents of Airnbn review data to the Dataset created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add documents\n",
    "documents = []\n",
    "for review in reviews[0:1000].fillna({'reviewer':'','comments':'','host':''}).itertuples(index=False):\n",
    "    document = ibmwex.Document(id=str(review.id), fields={'date':review.date, 'reviewer':review.reviewer, 'comments':review.comments, 'host':review.host}, tag='airbnb_data/boston_reviews')\n",
    "    documents.append(document)\n",
    "    \n",
    "dataset = api_datasets.add_document(dataset.id, documents)\n",
    "\n",
    "print(dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='locate_enrichment'></a>\n",
    "## Locate Part-of-Speech and Sentiment enrichments\n",
    "The next step in performing sentiment analysis will be to analyze the `comments` field and extract linguistic attributes and sentiment expressions. Use the WEX library api's as demonstrated in the python code below to extract words, phrases, and sentiment expressions, retrieve Part-of-Speech and Sentiment enrichment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Retrieve PoS and Sentiment enrichments\n",
    "pos = None\n",
    "sentiment = None\n",
    "api_enrichments = ibmwex.EnrichmentApi(api_client)\n",
    "for enrichment in api_enrichments.list().items:\n",
    "    if enrichment.type == 'pos':\n",
    "        pos = enrichment\n",
    "    elif enrichment.type == 'sentiment':\n",
    "        sentiment = enrichment\n",
    "\n",
    "print(pos.name)\n",
    "print(sentiment.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='create_collection'></a>\n",
    "## Create a collection\n",
    "A collection is defined as a source dataset and set of enrichments to be applied to the dataset. The documents in the source dataset are enriched through natural langauge processing known as enrichments.\n",
    "\n",
    "In this step, you will apply Part-of-Speech and Sentiment enrichments to the `comments` field to extract reviewer sentiment expressions about hosts. The following python code creates a collection named 'Airbnb Data : Boston Reviews' with Part-of-Speech and Sentiment enrichments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "api_collections = ibmwex.CollectionApi(api_client)\n",
    "collection = ibmwex.Collection()\n",
    "# Collection name\n",
    "collection.name = 'Airbnb Data : Boston Reviews'\n",
    "# Link the dataset\n",
    "collection.datasets = [dataset.id]\n",
    "# Link PoS and Sentiment enrichments\n",
    "collection.enrichments = [pos.id, sentiment.id]\n",
    "# Use the comment field as document body\n",
    "collection.enrich_field_groups = [ibmwex.EnrichFieldGroup(enrichments=[pos.id, sentiment.id], fields=['comments'])]\n",
    "collection.tags = {'defaultBodyFieldId':'comments'}\n",
    "# Define facets tree\n",
    "collection.category = pos.category\n",
    "collection.category.children.extend(sentiment.category.children)\n",
    "# Create a collection\n",
    "collection = api_collections.create(collection)\n",
    "\n",
    "print(collection.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since indexing is not instantaneous, the following code will wait until the timeout value of 10 minutes for the index creation to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_collection(api_collections, id, timeout_secs=600, period_secs=10):\n",
    "    mustend = time.time() + timeout_secs\n",
    "    while (time.time() < mustend):\n",
    "        completed = True\n",
    "        status = api_collections.status(id).docproc\n",
    "        completed = status.completed\n",
    "        if completed: return True\n",
    "\n",
    "        time.sleep(period_secs)\n",
    "    print(\"Time out\")\n",
    "    return False\n",
    "\n",
    "\n",
    "print('Wait until indexing is finished...')\n",
    "wait_collection(api_collections, collection.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='analyze_positive_target'></a>\n",
    "## Analyze positive targets\n",
    "\n",
    "The following code uses the collection that has been created so far to generate and display a positive target facet pair heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_dim = 'host'\n",
    "x_label = 'Host'\n",
    "y_dim = 'annotation._sentiment.target.positive'\n",
    "y_label = 'Positive Target'\n",
    "\n",
    "api_exploration = ibmwex.ExplorationApi(api_client)\n",
    "results = api_exploration.query(collection.id, '*:*', rows=0, facet='true', \n",
    "    facet_field=[x_dim,y_dim], facet_stats='pairs', facet_limit=15)\n",
    "\n",
    "fs = results.facet_stats\n",
    "\n",
    "x_labels = list(map(lambda i: i['id'], fs.dims[x_dim]))\n",
    "y_labels = list(map(lambda i: i['id'], fs.dims[y_dim]))\n",
    "correlation_matrix = list(\n",
    "    map(lambda  x : list(map(lambda y : y['correlation'], x)),\n",
    "        list(map(lambda x : x[y_dim], fs.stats['pairs'][x_dim]))\n",
    "    )\n",
    ")\n",
    "\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "sns.heatmap(correlation_matrix, xticklabels=y_labels, yticklabels=x_labels, cbar=False, cmap='Blues')\n",
    "ax.set(xlabel=y_label, ylabel=x_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='summary'></a>\n",
    "## Summary\n",
    "\n",
    "In this notebook you have learned how to use Watson Explorer from Watson Studio to analyze unstructured content to perform sentiment analysis using data sets for that are directly obtained real world data from the Airbnb's guest reviews from their lodging experience. You learned the basic steps to analyze the unstructured data through the Watson Explorer and narrow the focus on your unstructured data using python, python and WEX libraries, and create sentiment heatmaps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
