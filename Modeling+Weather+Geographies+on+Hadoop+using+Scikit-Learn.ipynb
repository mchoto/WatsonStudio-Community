{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling Weather Geographies using Scikit-Learn on Hadoop Data\n",
    "\n",
    "In this notebook, we are using aggregated `TMAX` data by global weather stations in order to create a machine learning model to predict the daily maximum temperatures at any given latitude and longitude. Our model will take 3 continuous predictors: latitude, longitude, and elevation, and provide an estimated `TMAX` for a given day of the year.\n",
    "\n",
    "Our input data will reside in HDFS for a registered Hadoop Integration system. To avoid having to copy the data from Hadoop into Watson Studio, we will use a remote Livy session to build the model _within Hadoop itself_. Then we will \"pull\" the model into Watson Studio and save it to your Watson Studio filesystem, making it available for use with other Watson Studio model management features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">Note: The Scikit-Learn APIs that we use in this sample are single-threaded; this sample does *not* use Hadoop distributed computing to create the model. The reason we're using Hadoop in this example is purely for data purposes: we want to build the model <i>where the data resides</i>, instead of having to pull the data into Watson Studio.</div>\n",
    "\n",
    "## Table of Contents\n",
    "This notebook contains these main sections:\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Create a Remote Livy Session](#Create_Livy_Session)\n",
    "3. [The Data](#The_Data)\n",
    "4. [The Model](#The_Model)\n",
    "5. [Copy Models to Watson Studio Local](#Model_Copy_to_WSL)\n",
    "6. [Save Models to Watson Studio Filesystem](#Save_Models_to_WSL_Filesystem)\n",
    "7. [Cleanup the_Remote Livy Session](#Cleanup_Remote_Livy_Session)\n",
    "8. [Summary](#Summary)\n",
    "\n",
    "<a id='Prerequisites'></a>\n",
    "## Prerequisites\n",
    "\n",
    "### I) Create a custom image with `tqdm`\n",
    "The model creation logic for this sample requires the python `tqdm` library. Since we will be running the model creation logic in a _remote_ Livy session, we will need to create a custom image which includes `tqdm`, and then configure our Livy session to use that image. In order to do this you can take the following steps:\n",
    "\n",
    "#### A. Start an environment\n",
    "From your project home page, use the `Environments` tab to _start_ a \"`Jupyter with Python 2.7, ...`\" environment.\n",
    "\n",
    "#### B. Install `tqdm` into the environment\n",
    "From your project home page, use the `Environments` tab to _launch a terminal_ shell for the environment that you started in Step A. When you are inside the terminal, type the following command to install `tqdm`:\n",
    "\n",
    "```\n",
    "conda install tqdm -y\n",
    "```\n",
    "\n",
    "When the command completes, you can `exit` the terminal.\n",
    "\n",
    "#### C. Save the environment as a custom image\n",
    "From your project home page, use the `Environments` tab to _save_ the environment that you edited in Step B.\n",
    "\n",
    "### II) Register a Hadoop Integration system (Admin)\n",
    "\n",
    "Ask your Watson Studio admin to use the **Admin Console => Hadoop Integration** option to:\n",
    "\n",
    "  * Register a Hadoop Integration system.\n",
    "\n",
    "    ** NOTE: Installation and configuration of IBM's Hadoop Integration (`HI`) service on a Hadoop cluster must be done by a Hadoop admin _before_ that system can be registered with your Watson Studio. **\n",
    "\n",
    "\n",
    "  * Push the custom image you created in Step I above to the registered HI system.\n",
    "  \n",
    "When your admin indicates that your custom image has been pushed to the registered HI system, you can proceed with this sample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports needed for the cells which run locally on Watson Studio.\n",
    "import dsx_core_utils\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Create_Livy_Session'></a>\n",
    "## Create a Remote Livy Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, let's get a list of registered Hadoop Integration systems. Look for a system that has the `imageId` of your custom image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DSXHI_SYSTEMS = dsx_core_utils.get_dsxhi_info(showSummary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Configure the spark session that we will use when running on the selected registered HI system. In this case we want the session to start with 2G memory and we only need a single executor since we're using single-threaded Scikit-Learn APIs. **NOTE**: `myConfig` here is optional; if you prefer to use default configs you can omit this cell and remove the `addlConfig` argument in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"2G\",\n",
    " \"numExecutors\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://zinc1.fyre.ibm.com:8443/gateway/dsx-loc-chell-375-master-1/livy2/v1 with image Jupyter with Python 2.7 + TQDM\n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "# Set up sparkmagic to connect to the selected registered HI\n",
    "# system with the specified configs. **NOTE** This notebook\n",
    "# requires Spark 2, so you should set 'livy' to 'livyspark2'.\n",
    "HI_CONFIG = dsx_core_utils.setup_livy_sparkmagic(\n",
    "  system=\"Zinc\", \n",
    "  livy=\"livyspark2\",\n",
    "  imageId=\"py27-tqdm\",\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load spark magic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's capture some state about the configured Hadoop Integraton system, to be used later in this notebook. Then start up a new, remote Livy session to connect to that HI system. **NOTE**: Depending on a) the resources available in the remote Hadoop system and b) the speed of your cluster, attempts to start the session might report errors due to timeout or due to a session coming up `dead`.  In such cases you should run **`%spark cleanup`** as a separate cell, then re-run this cell again.  If session creation continues to fail, contact the Hadoop admin of the target Hadoop cluster to see if everything is configured as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>230</td><td>application_1533478912530_1339</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ales1.fyre.ibm.com:8088/proxy/application_1533478912530_1339/\">Link</a></td><td><a target=\"_blank\" href=\"http://ales3.fyre.ibm.com:8042/node/containerlogs/container_e32_1533478912530_1339_01_000001/user1\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'sksess1'\n",
    "livy_endpoint = HI_CONFIG['LIVY']\n",
    "webhdfs_endpoint = HI_CONFIG['WEBHDFS']\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For reference / debugging: Print out the name of the Hadoop node to which the remote session has been assigned. When \"local\" files are created within the remote session, they will be written to this node. All of the Yarn container artifacts (workspace and temp files) will exist on this node as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote Livy session driver: ales3.fyre.ibm.com"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "import socket\n",
    "print(\"Remote Livy session driver: {}\".format(socket.gethostname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following cell, and all subsequent cells which have **`%%spark`** as their first line, will run *remotely*, i.e. within a Yarn container that exists on the registered Hadoop Integration system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "# Declare imports needed for all of the cells that will run remotely.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import getpass, time, os\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load IBM Hadoop Integration utilities to facilitate remote functionality.\n",
    "hi_utils_lib = os.getenv(\"HI_UTILS_PATH\", \"\")\n",
    "sc.addPyFile(\"hdfs://{}\".format(hi_utils_lib))\n",
    "import hi_core_utils\n",
    "\n",
    "# Declare some target directory paths that will be used when dealing with data and model artifacts.\n",
    "input_csv_name = \"seasonal_data.csv\"\n",
    "hdfs_dataset_dir = \"/user/{}/datasets\".format(getpass.getuser())\n",
    "input_ds = \"{}/{}\".format(hdfs_dataset_dir, input_csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='The_Data'></a>\n",
    "## The Data\n",
    "Load our test data into HDFS. For purposes of this sample our data is small and comes from a remote URL. We do not _need_ to put it into HDFS--but we choose to do so for demonstration purposes. In a real scenario the desired data should already be loaded into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "# Download the data file to the local fs of the Hadoop node that is \"driving\"\n",
    "# our active livy session.\n",
    "sc.addFile(\"https://raw.githubusercontent.com/IBMDataScience/DSX-DemoCenter/master/weatherGeographies/data_assets/seasonal_data.csv\")\n",
    "\n",
    "# Create a target directory on HDFS.\n",
    "hi_core_utils.run_command(\"hdfs dfs -mkdir -p {}\".format(hdfs_dataset_dir))\n",
    "\n",
    "# Upload the saved csv file to HDFS.\n",
    "hi_core_utils.run_command(\"hdfs dfs -put -f {}/{} {}\".format(\n",
    "    SparkFiles.getRootDirectory(), input_csv_name, hdfs_dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now use spark to read the data, as a **spark data frame**, from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "df_data_1 = spark.read.format(\n",
    "    \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\n",
    "    \"header\", \"true\").option(\"inferSchema\", \"true\").load(input_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='The_Model'></a>\n",
    "## The Model\n",
    "Let's first split the data by columns into features and response variables. **`sdf_x`** and **`sdf_y`** will hold **spark dataframes** at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "sdf_x = df_data_1[['elevation','latitude','longitude']]\n",
    "sdf_y = df_data_1[['21-Mar','21-Jun','21-Sep','21-Dec']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Convert the spark data frames to pandas since the Scikit-Learn APIs that we use have to work with pandas (and therefore they are _not_ distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "x = sdf_x.toPandas()\n",
    "y = sdf_y.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split the data further into training and testing sets. We will fit a **Gradient Boosting Regressor** model. We should first tune our model for the hyperparameter `n_estimators` to discover what is the maximum number of estimators we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "x_init, x_test, y_init, y_test = train_test_split(x, y['21-Jun'], test_size=.25)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_init, y_init, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can iterate over a specified range to determine the best value for n_estimators. **Note**: Since we're running inside of a _remote_ Livy session, UI-based functionality will not work. Thus you will *not* see the progress indicator bar that you might expect to see if you were running this code locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "n_est = list(range(1,201))\n",
    "val_mad = list()\n",
    "for n in tqdm(n_est):\n",
    "    val_model = GradientBoostingRegressor(n_estimators=n)\n",
    "    val_model.fit(x_train,y_train)\n",
    "    val_pred = val_model.predict(x_val)\n",
    "    val_mad.append(mean_absolute_error(y_val,val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Finding the Elbow\n",
    "In order to find the optimal value of `n_estimators`, we'll calculate the equation of the line between our minimum and maximum value of `n_estimators`, and find out the value of `n` in which the validation error is the furthest away from the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of n is 33."
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "slope = (val_mad[-1]-val_mad[0])/(n_est[-1]-n_est[0])\n",
    "intercept = val_mad[0] - slope*n_est[0]\n",
    "best_n = (np.array(n_est)*slope+intercept-np.array(val_mad)).argmax()\n",
    "print(\"The best value of n is {}.\".format(n_est[best_n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the best number of estimators from above, we'll fit the model we'll use for prediction on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=33, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "gbtr = GradientBoostingRegressor(n_estimators=n_est[best_n])\n",
    "gbtr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2.0153655727\n",
      "R^2 value: 0.782325989428"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "y_pred = gbtr.predict(x_test)\n",
    "print(\"Mean Absolute Error: {}\\nR^2 value: {}\".format(mean_absolute_error(y_pred,y_test),gbtr.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Remember that we are dealing with degrees Celsius. Our mean error in this case is around 2 degrees. We also have a strong $R^2$ value.\n",
    "\n",
    "Now let's fit the models on the entirety of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=33, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "gbtr.fit(x,y['21-Jun'])\n",
    "gbtr_dec = GradientBoostingRegressor(n_estimators=n_est[best_n])\n",
    "gbtr_dec.fit(x,y['21-Dec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Model_Copy_to_WSL'></a>\n",
    "## Copy Models to Watson Studio Local\n",
    "\n",
    "The models now exist within the memory of the remote livy session. In order to use them in Watson Studio model management, we need to copy them to the local Watson Studio environment.  This is done in two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Write Models to HDFS\n",
    "First, in the _remote_ session, we use a Hadoop Integration utility method to write the models to HDFS, along with some associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'hdfs:///user/user1/.dsxhi/models/gbtr_jun/2/model', 'version': 2, 'name': 'gbtr_jun', 'latest_version': 2}\n",
      "{'path': 'hdfs:///user/user1/.dsxhi/models/gbtr_dec/2/model', 'version': 2, 'name': 'gbtr_dec', 'latest_version': 2}\n",
      "/hadoop/yarn/local/usercache/user1/appcache/application_1533478912530_1339/container_e32_1533478912530_1339_01_000001/py27-tqdm.tar.gz/conda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend."
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "print(hi_core_utils.write_model_to_hdfs(model=gbtr, model_name=\"gbtr_jun\"))\n",
    "print(hi_core_utils.write_model_to_hdfs(model=gbtr_dec, model_name=\"gbtr_dec\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load Models from HDFS into Watson Studio Local\n",
    "Then, on the Watson Studio _local_ side, we use a Watson Studio Local utility method to load the model from HDFS into memory. Note that the model names we use here should match the ones we used in the previous cell, when we wrote the models to HDFS.\n",
    "\n",
    "Note also that this cell **does not** begin with the **`%%spark`** line, which means it is running locally in your Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from hdfs:///user/user1/.dsxhi/models/gbtr_jun/2/model\n",
      "Model loaded from hdfs:///user/user1/.dsxhi/models/gbtr_dec/2/model\n"
     ]
    }
   ],
   "source": [
    "loc_gbtr_jun = dsx_core_utils.load_model_from_hdfs(webhdfs_endpoint, model_name=\"gbtr_jun\")\n",
    "loc_gbtr_dec = dsx_core_utils.load_model_from_hdfs(webhdfs_endpoint, model_name=\"gbtr_dec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Save_Models_to_WSL_Filesystem'></a>\n",
    "## Save Models to Watson Studio Filesystem\n",
    "We can now save `scikit-learn` models to the Watson Studio filesystem for publishing, scoring, deployment, and evaluations.\n",
    "\n",
    "When invoking the `save` function we want to pass pandas dataframes for **`x_test`** and **`y_test`** as arguments. By doing so we allow the `save` function to a) determine the schema of the test data automatically, and b) find an example row that can be used elsewhere in the WSL model management UI (ex. for real-time scoring).\n",
    "\n",
    "At this point the desired dataframes exist within the _remote_ Livy session, which means they are not directly accessible from the local notebook session. However, we can use `sparkmagic` to pull a **single** row (\"`-n 1`\") from each of those views.  This allows us to get the minimum necessary information we need from the test data **without** having to read the full datasets from HDFS.\n",
    "\n",
    "Here we load one row of data from each dataframe into **`x`** and **`y`**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name -n 1 -o x\n",
    "x = sdf_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name -n 1 -o y\n",
    "y = sdf_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have our **`x_test`** and **`y_test`** dataframes, let's import the `save` function from the `dsx_ml.ml` library. The save function takes a few additional arguments which are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from dsx_ml.ml import save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can save both the June 21 and December 21 models.\n",
    "\n",
    "**NOTE**: Since we're using dataframes with a **single** row, i.e. partial data, we choose to skip calculation of performance metrics for the saved model (\"`skip_metrics = True`\") since metrics based on a single row are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'path': '/user-home/1001/DSX_Projects/Models on Hadoop/models/Jun21 Scikit via Hadoop/17',\n",
       " 'scoring_endpoint': 'https://dsxl-api/v3/project/score/Python27/scikit-learn-0.19/Models%20on%20Hadoop/Jun21%20Scikit%20via%20Hadoop/17'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(model = loc_gbtr_jun,\n",
    "     name = 'Jun21 Scikit via Hadoop',\n",
    "     x_test = x,\n",
    "     y_test = pd.DataFrame(y['21-Jun']),\n",
    "     algorithm_type = 'Regression',\n",
    "     skip_metrics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/user-home/1001/DSX_Projects/Models on Hadoop/models/Dec21 Scikit via Hadoop/17',\n",
       " 'scoring_endpoint': 'https://dsxl-api/v3/project/score/Python27/scikit-learn-0.19/Models%20on%20Hadoop/Dec21%20Scikit%20via%20Hadoop/17'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(model = loc_gbtr_dec,\n",
    "     name = 'Dec21 Scikit via Hadoop',\n",
    "     x_test = x,\n",
    "     y_test = pd.DataFrame(y['21-Dec']),\n",
    "     algorithm_type = 'Regression',\n",
    "     skip_metrics = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Cleanup_Remote_Livy_Session'></a>\n",
    "## Cleanup the Remote Livy Session\n",
    "We're done with our models and we have successfully saved them to Watson Studio. Let's cleanup our remote Livy session. \n",
    "This will terminate the session and release resources back to the remote Hadoop Integration system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Summary'></a>\n",
    "## Summary\n",
    "In this notebook you learned how to create a `scikit-learn` model _on a registered Hadoop Integration system_, allowing you to create the model where the data resides, instead of having to copy your data into the Watson Studio environment.  Once the model was created you were able to save it in the Watson Studio environment, where it can now be used as input for other Watson Studio model management features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">Note: To save resources and get the best performance please use the code below to stop the kernel before exiting your notebook.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2018. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
